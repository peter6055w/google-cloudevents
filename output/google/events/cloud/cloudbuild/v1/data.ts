// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.7
//   protoc               v6.32.0
// source: google/events/cloud/cloudbuild/v1/data.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../../protobuf/duration";
import { Timestamp } from "../../../../protobuf/timestamp";

export const protobufPackage = "google.events.cloud.cloudbuild.v1";

/** Build event data for Google Cloud Platform API operations. */
export interface BuildEventData {
  /** Unique identifier of the build. */
  id: string;
  /** ID of the project. */
  projectId: string;
  /** Status of the build. */
  status: BuildEventData_Status;
  /** Customer-readable message about the current status. */
  statusDetail: string;
  /** The location of the source files to build. */
  source?:
    | Source
    | undefined;
  /** The operations to be performed on the workspace. */
  steps: BuildStep[];
  /** Results of the build. */
  results?:
    | Results
    | undefined;
  /** Time at which the request to create the build was received. */
  createTime?:
    | Date
    | undefined;
  /** Time at which execution of the build was started. */
  startTime?:
    | Date
    | undefined;
  /**
   * Time at which execution of the build was finished.
   *
   * The difference between finish_time and start_time is the duration of the
   * build's execution.
   */
  finishTime?:
    | Date
    | undefined;
  /**
   * Amount of time that this build should be allowed to run, to second
   * granularity. If this amount of time elapses, work on the build will cease
   * and the build status will be `TIMEOUT`.
   */
  timeout?:
    | Duration
    | undefined;
  /**
   * A list of images to be pushed upon the successful completion of all build
   * steps.
   *
   * The images are pushed using the builder service account's credentials.
   *
   * The digests of the pushed images will be stored in the `Build` resource's
   * results field.
   *
   * If any of the images fail to be pushed, the build status is marked
   * `FAILURE`.
   */
  images: string[];
  /**
   * TTL in queue for this build. If provided and the build is enqueued longer
   * than this value, the build will expire and the build status will be
   * `EXPIRED`.
   *
   * The TTL starts ticking from create_time.
   */
  queueTtl?:
    | Duration
    | undefined;
  /**
   * Artifacts produced by the build that should be uploaded upon
   * successful completion of all build steps.
   */
  artifacts?:
    | Artifacts
    | undefined;
  /**
   * Google Cloud Storage bucket where logs should be written (see
   * [Bucket Name
   * Requirements](https://cloud.google.com/storage/docs/bucket-naming#requirements)).
   * Logs file names will be of the format `${logs_bucket}/log-${build_id}.txt`.
   */
  logsBucket: string;
  /** A permanent fixed identifier for source. */
  sourceProvenance?:
    | SourceProvenance
    | undefined;
  /**
   * The ID of the `BuildTrigger` that triggered this build, if it
   * was triggered automatically.
   */
  buildTriggerId: string;
  /** Special options for this build. */
  options?:
    | BuildOptions
    | undefined;
  /** URL to logs for this build in Google Cloud Console. */
  logUrl: string;
  /** Substitutions data for `Build` resource. */
  substitutions: { [key: string]: string };
  /** Tags for annotation of a `Build`. These are not docker tags. */
  tags: string[];
  /** Secrets to decrypt using Cloud Key Management Service. */
  secrets: Secret[];
  /**
   * Stores timing information for phases of the build. Valid keys
   * are:
   *
   * * BUILD: time to execute all build steps
   * * PUSH: time to push all specified images.
   * * FETCHSOURCE: time to fetch source.
   *
   * If the build does not specify source or images,
   * these keys will not be included.
   */
  timing: { [key: string]: TimeSpan };
}

/** Possible status of a build or build step. */
export enum BuildEventData_Status {
  /** STATUS_UNKNOWN - Status of the build is unknown. */
  STATUS_UNKNOWN = 0,
  /** QUEUED - Build or step is queued; work has not yet begun. */
  QUEUED = 1,
  /** WORKING - Build or step is being executed. */
  WORKING = 2,
  /** SUCCESS - Build or step finished successfully. */
  SUCCESS = 3,
  /** FAILURE - Build or step failed to complete successfully. */
  FAILURE = 4,
  /** INTERNAL_ERROR - Build or step failed due to an internal cause. */
  INTERNAL_ERROR = 5,
  /** TIMEOUT - Build or step took longer than was allowed. */
  TIMEOUT = 6,
  /** CANCELLED - Build or step was canceled by a user. */
  CANCELLED = 7,
  /** EXPIRED - Build was enqueued for longer than the value of `queue_ttl`. */
  EXPIRED = 9,
  UNRECOGNIZED = -1,
}

export function buildEventData_StatusFromJSON(object: any): BuildEventData_Status {
  switch (object) {
    case 0:
    case "STATUS_UNKNOWN":
      return BuildEventData_Status.STATUS_UNKNOWN;
    case 1:
    case "QUEUED":
      return BuildEventData_Status.QUEUED;
    case 2:
    case "WORKING":
      return BuildEventData_Status.WORKING;
    case 3:
    case "SUCCESS":
      return BuildEventData_Status.SUCCESS;
    case 4:
    case "FAILURE":
      return BuildEventData_Status.FAILURE;
    case 5:
    case "INTERNAL_ERROR":
      return BuildEventData_Status.INTERNAL_ERROR;
    case 6:
    case "TIMEOUT":
      return BuildEventData_Status.TIMEOUT;
    case 7:
    case "CANCELLED":
      return BuildEventData_Status.CANCELLED;
    case 9:
    case "EXPIRED":
      return BuildEventData_Status.EXPIRED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BuildEventData_Status.UNRECOGNIZED;
  }
}

export function buildEventData_StatusToJSON(object: BuildEventData_Status): string {
  switch (object) {
    case BuildEventData_Status.STATUS_UNKNOWN:
      return "STATUS_UNKNOWN";
    case BuildEventData_Status.QUEUED:
      return "QUEUED";
    case BuildEventData_Status.WORKING:
      return "WORKING";
    case BuildEventData_Status.SUCCESS:
      return "SUCCESS";
    case BuildEventData_Status.FAILURE:
      return "FAILURE";
    case BuildEventData_Status.INTERNAL_ERROR:
      return "INTERNAL_ERROR";
    case BuildEventData_Status.TIMEOUT:
      return "TIMEOUT";
    case BuildEventData_Status.CANCELLED:
      return "CANCELLED";
    case BuildEventData_Status.EXPIRED:
      return "EXPIRED";
    case BuildEventData_Status.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface BuildEventData_SubstitutionsEntry {
  key: string;
  value: string;
}

export interface BuildEventData_TimingEntry {
  key: string;
  value?: TimeSpan | undefined;
}

export interface Source {
  /** If provided, get the source from this location in Google Cloud Storage. */
  storageSource?:
    | StorageSource
    | undefined;
  /**
   * If provided, get the source from this location in a Cloud Source
   * Repository.
   */
  repoSource?: RepoSource | undefined;
}

/** Location of the source in an archive file in Google Cloud Storage. */
export interface StorageSource {
  /**
   * Google Cloud Storage bucket containing the source (see
   * [Bucket Name
   * Requirements](https://cloud.google.com/storage/docs/bucket-naming#requirements)).
   */
  bucket: string;
  /** Google Cloud Storage object containing the source. */
  object: string;
  /**
   * Google Cloud Storage generation for the object. If the generation is
   * omitted, the latest generation will be used.
   */
  generation: Long;
}

/** Location of the source in a Google Cloud Source Repository. */
export interface RepoSource {
  /** ID of the project that owns the Cloud Source Repository. */
  projectId: string;
  /** Name of the Cloud Source Repository. */
  repoName: string;
  /**
   * Regex matching branches to build.
   *
   * The syntax of the regular expressions accepted is the syntax accepted by
   * RE2 and described at https://github.com/google/re2/wiki/Syntax
   */
  branchName?:
    | string
    | undefined;
  /**
   * Regex matching tags to build.
   *
   * The syntax of the regular expressions accepted is the syntax accepted by
   * RE2 and described at https://github.com/google/re2/wiki/Syntax
   */
  tagName?:
    | string
    | undefined;
  /** Explicit commit SHA to build. */
  commitSha?:
    | string
    | undefined;
  /**
   * Directory, relative to the source root, in which to run the build.
   *
   * This must be a relative path. If a step's `dir` is specified and is an
   * absolute path, this value is ignored for that step's execution.
   */
  dir: string;
  /**
   * Only trigger a build if the revision regex does NOT match the revision
   * regex.
   */
  invertRegex: boolean;
  /**
   * Substitutions to use in a triggered build.
   * Should only be used with RunBuildTrigger
   */
  substitutions: { [key: string]: string };
}

export interface RepoSource_SubstitutionsEntry {
  key: string;
  value: string;
}

/** A step in the build pipeline. */
export interface BuildStep {
  /**
   * The name of the container image that will run this particular
   * build step.
   *
   * If the image is available in the host's Docker daemon's cache, it
   * will be run directly. If not, the host will attempt to pull the image
   * first, using the builder service account's credentials if necessary.
   *
   * The Docker daemon's cache will already have the latest versions of all of
   * the officially supported build steps
   * ([https://github.com/GoogleCloudPlatform/cloud-builders](https://github.com/GoogleCloudPlatform/cloud-builders)).
   * The Docker daemon will also have cached many of the layers for some popular
   * images, like "ubuntu", "debian", but they will be refreshed at the time you
   * attempt to use them.
   *
   * If you built an image in a previous build step, it will be stored in the
   * host's Docker daemon's cache and is available to use as the name for a
   * later build step.
   */
  name: string;
  /**
   * A list of environment variable definitions to be used when running a step.
   *
   * The elements are of the form "KEY=VALUE" for the environment variable "KEY"
   * being given the value "VALUE".
   */
  env: string[];
  /**
   * A list of arguments that will be presented to the step when it is started.
   *
   * If the image used to run the step's container has an entrypoint, the `args`
   * are used as arguments to that entrypoint. If the image does not define
   * an entrypoint, the first element in args is used as the entrypoint,
   * and the remainder will be used as arguments.
   */
  args: string[];
  /**
   * Working directory to use when running this step's container.
   *
   * If this value is a relative path, it is relative to the build's working
   * directory. If this value is absolute, it may be outside the build's working
   * directory, in which case the contents of the path may not be persisted
   * across build step executions, unless a `volume` for that path is specified.
   *
   * If the build specifies a `RepoSource` with `dir` and a step with a `dir`,
   * which specifies an absolute path, the `RepoSource` `dir` is ignored for
   * the step's execution.
   */
  dir: string;
  /**
   * Unique identifier for this build step, used in `wait_for` to
   * reference this build step as a dependency.
   */
  id: string;
  /**
   * The ID(s) of the step(s) that this build step depends on.
   * This build step will not start until all the build steps in `wait_for`
   * have completed successfully. If `wait_for` is empty, this build step will
   * start when all previous build steps in the `Build.Steps` list have
   * completed successfully.
   */
  waitFor: string[];
  /**
   * Entrypoint to be used instead of the build step image's default entrypoint.
   * If unset, the image's default entrypoint is used.
   */
  entrypoint: string;
  /**
   * A list of environment variables which are encrypted using a Cloud Key
   * Management Service crypto key. These values must be specified in the
   * build's `Secret`.
   */
  secretEnv: string[];
  /**
   * List of volumes to mount into the build step.
   *
   * Each volume is created as an empty volume prior to execution of the
   * build step. Upon completion of the build, volumes and their contents are
   * discarded.
   *
   * Using a named volume in only one step is not valid as it is indicative
   * of a build request with an incorrect configuration.
   */
  volumes: Volume[];
  /** Stores timing information for executing this build step. */
  timing?:
    | TimeSpan
    | undefined;
  /**
   * Stores timing information for pulling this build step's
   * builder image only.
   */
  pullTiming?:
    | TimeSpan
    | undefined;
  /**
   * Time limit for executing this build step. If not defined, the step has no
   * time limit and will be allowed to continue to run until either it completes
   * or the build itself times out.
   */
  timeout?:
    | Duration
    | undefined;
  /**
   * Status of the build step. At this time, build step status is
   * only updated on build completion; step status is not updated in real-time
   * as the build progresses.
   */
  status: BuildEventData_Status;
}

/**
 * Volume describes a Docker container volume which is mounted into build steps
 * in order to persist files across build step execution.
 */
export interface Volume {
  /**
   * Name of the volume to mount.
   *
   * Volume names must be unique per build step and must be valid names for
   * Docker volumes. Each named volume must be used by at least two build steps.
   */
  name: string;
  /**
   * Path at which to mount the volume.
   *
   * Paths must be absolute and cannot conflict with other volume paths on the
   * same build step or with certain reserved volume paths.
   */
  path: string;
}

/** Artifacts created by the build pipeline. */
export interface Results {
  /** Container images that were built as a part of the build. */
  images: BuiltImage[];
  /**
   * List of build step digests, in the order corresponding to build step
   * indices.
   */
  buildStepImages: string[];
  /** Path to the artifact manifest. Only populated when artifacts are uploaded. */
  artifactManifest: string;
  /** Number of artifacts uploaded. Only populated when artifacts are uploaded. */
  numArtifacts: Long;
  /**
   * List of build step outputs, produced by builder images, in the order
   * corresponding to build step indices.
   *
   * [Cloud Builders](https://cloud.google.com/cloud-build/docs/cloud-builders)
   * can produce this output by writing to `$BUILDER_OUTPUT/output`.
   * Only the first 4KB of data is stored.
   */
  buildStepOutputs: Uint8Array[];
  /** Time to push all non-container artifacts. */
  artifactTiming?: TimeSpan | undefined;
}

/** An image built by the pipeline. */
export interface BuiltImage {
  /**
   * Name used to push the container image to Google Container Registry, as
   * presented to `docker push`.
   */
  name: string;
  /** Docker Registry 2.0 digest. */
  digest: string;
  /** Stores timing information for pushing the specified image. */
  pushTiming?: TimeSpan | undefined;
}

/**
 * Artifacts produced by a build that should be uploaded upon
 * successful completion of all build steps.
 */
export interface Artifacts {
  /**
   * A list of images to be pushed upon the successful completion of all build
   * steps.
   *
   * The images will be pushed using the builder service account's credentials.
   *
   * The digests of the pushed images will be stored in the Build resource's
   * results field.
   *
   * If any of the images fail to be pushed, the build is marked FAILURE.
   */
  images: string[];
  /**
   * A list of objects to be uploaded to Cloud Storage upon successful
   * completion of all build steps.
   *
   * Files in the workspace matching specified paths globs will be uploaded to
   * the specified Cloud Storage location using the builder service account's
   * credentials.
   *
   * The location and generation of the uploaded objects will be stored in the
   * Build resource's results field.
   *
   * If any objects fail to be pushed, the build is marked FAILURE.
   */
  objects?: Artifacts_ArtifactObjects | undefined;
}

/**
 * Files in the workspace to upload to Cloud Storage upon successful
 * completion of all build steps.
 */
export interface Artifacts_ArtifactObjects {
  /**
   * Cloud Storage bucket and optional object path, in the form
   * "gs://bucket/path/to/somewhere/". (see [Bucket Name
   * Requirements](https://cloud.google.com/storage/docs/bucket-naming#requirements)).
   *
   * Files in the workspace matching any path pattern will be uploaded to
   * Cloud Storage with this location as a prefix.
   */
  location: string;
  /** Path globs used to match files in the build's workspace. */
  paths: string[];
  /** Stores timing information for pushing all artifact objects. */
  timing?: TimeSpan | undefined;
}

/** Start and end times for a build execution phase. */
export interface TimeSpan {
  /** Start of time span. */
  startTime?:
    | Date
    | undefined;
  /** End of time span. */
  endTime?: Date | undefined;
}

/**
 * Provenance of the source. Ways to find the original source, or verify that
 * some source was used for this build.
 */
export interface SourceProvenance {
  /**
   * A copy of the build's `source.storage_source`, if exists, with any
   * generations resolved.
   */
  resolvedStorageSource?:
    | StorageSource
    | undefined;
  /**
   * A copy of the build's `source.repo_source`, if exists, with any
   * revisions resolved.
   */
  resolvedRepoSource?:
    | RepoSource
    | undefined;
  /**
   * Hash(es) of the build source, which can be used to verify that
   * the original source integrity was maintained in the build. Note that
   * `FileHashes` will only be populated if `BuildOptions` has requested a
   * `SourceProvenanceHash`.
   *
   * The keys to this map are file paths used as build source and the values
   * contain the hash values for those files.
   *
   * If the build source came in a single package such as a gzipped tarfile
   * (`.tar.gz`), the `FileHash` will be for the single path to that file.
   */
  fileHashes: { [key: string]: FileHashes };
}

export interface SourceProvenance_FileHashesEntry {
  key: string;
  value?: FileHashes | undefined;
}

/**
 * Container message for hashes of byte content of files, used in
 * SourceProvenance messages to verify integrity of source input to the build.
 */
export interface FileHashes {
  /** Collection of file hashes. */
  fileHash: Hash[];
}

/** Container message for hash values. */
export interface Hash {
  /** The type of hash that was performed. */
  type: Hash_HashType;
  /** The hash value. */
  value: Uint8Array;
}

/** Specifies the hash algorithm, if any. */
export enum Hash_HashType {
  /** NONE - No hash requested. */
  NONE = 0,
  /** SHA256 - Use a sha256 hash. */
  SHA256 = 1,
  /** MD5 - Use a md5 hash. */
  MD5 = 2,
  UNRECOGNIZED = -1,
}

export function hash_HashTypeFromJSON(object: any): Hash_HashType {
  switch (object) {
    case 0:
    case "NONE":
      return Hash_HashType.NONE;
    case 1:
    case "SHA256":
      return Hash_HashType.SHA256;
    case 2:
    case "MD5":
      return Hash_HashType.MD5;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Hash_HashType.UNRECOGNIZED;
  }
}

export function hash_HashTypeToJSON(object: Hash_HashType): string {
  switch (object) {
    case Hash_HashType.NONE:
      return "NONE";
    case Hash_HashType.SHA256:
      return "SHA256";
    case Hash_HashType.MD5:
      return "MD5";
    case Hash_HashType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Pairs a set of secret environment variables containing encrypted
 * values with the Cloud KMS key to use to decrypt the value.
 */
export interface Secret {
  /** Cloud KMS key name to use to decrypt these envs. */
  kmsKeyName: string;
  /**
   * Map of environment variable name to its encrypted value.
   *
   * Secret environment variables must be unique across all of a build's
   * secrets, and must be used by at least one build step. Values can be at most
   * 64 KB in size. There can be at most 100 secret values across all of a
   * build's secrets.
   */
  secretEnv: { [key: string]: Uint8Array };
}

export interface Secret_SecretEnvEntry {
  key: string;
  value: Uint8Array;
}

/** Optional arguments to enable specific features of builds. */
export interface BuildOptions {
  /** Requested hash for SourceProvenance. */
  sourceProvenanceHash: Hash_HashType[];
  /** Requested verifiability options. */
  requestedVerifyOption: BuildOptions_VerifyOption;
  /** Compute Engine machine type on which to run the build. */
  machineType: BuildOptions_MachineType;
  /**
   * Requested disk size for the VM that runs the build. Note that this is *NOT*
   * "disk free"; some of the space will be used by the operating system and
   * build utilities. Also note that this is the minimum disk size that will be
   * allocated for the build -- the build may run with a larger disk than
   * requested. At present, the maximum disk size is 1000GB; builds that request
   * more than the maximum are rejected with an error.
   */
  diskSizeGb: Long;
  /**
   * Option to specify behavior when there is an error in the substitution
   * checks.
   */
  substitutionOption: BuildOptions_SubstitutionOption;
  /**
   * Option to define build log streaming behavior to Google Cloud
   * Storage.
   */
  logStreamingOption: BuildOptions_LogStreamingOption;
  /**
   * Option to specify a `WorkerPool` for the build.
   * Format: projects/{project}/locations/{location}/workerPools/{workerPool}
   */
  workerPool: string;
  /**
   * Option to specify the logging mode, which determines where the logs are
   * stored.
   */
  logging: BuildOptions_LoggingMode;
  /**
   * A list of global environment variable definitions that will exist for all
   * build steps in this build. If a variable is defined in both globally and in
   * a build step, the variable will use the build step value.
   *
   * The elements are of the form "KEY=VALUE" for the environment variable "KEY"
   * being given the value "VALUE".
   */
  env: string[];
  /**
   * A list of global environment variables, which are encrypted using a Cloud
   * Key Management Service crypto key. These values must be specified in the
   * build's `Secret`. These variables will be available to all build steps
   * in this build.
   */
  secretEnv: string[];
  /**
   * Global list of volumes to mount for ALL build steps
   *
   * Each volume is created as an empty volume prior to starting the build
   * process. Upon completion of the build, volumes and their contents are
   * discarded. Global volume names and paths cannot conflict with the volumes
   * defined a build step.
   *
   * Using a global volume in a build with only one step is not valid as
   * it is indicative of a build request with an incorrect configuration.
   */
  volumes: Volume[];
}

/** Specifies the manner in which the build should be verified, if at all. */
export enum BuildOptions_VerifyOption {
  /** NOT_VERIFIED - Not a verifiable build. (default) */
  NOT_VERIFIED = 0,
  /** VERIFIED - Verified build. */
  VERIFIED = 1,
  UNRECOGNIZED = -1,
}

export function buildOptions_VerifyOptionFromJSON(object: any): BuildOptions_VerifyOption {
  switch (object) {
    case 0:
    case "NOT_VERIFIED":
      return BuildOptions_VerifyOption.NOT_VERIFIED;
    case 1:
    case "VERIFIED":
      return BuildOptions_VerifyOption.VERIFIED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BuildOptions_VerifyOption.UNRECOGNIZED;
  }
}

export function buildOptions_VerifyOptionToJSON(object: BuildOptions_VerifyOption): string {
  switch (object) {
    case BuildOptions_VerifyOption.NOT_VERIFIED:
      return "NOT_VERIFIED";
    case BuildOptions_VerifyOption.VERIFIED:
      return "VERIFIED";
    case BuildOptions_VerifyOption.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Supported VM sizes. */
export enum BuildOptions_MachineType {
  /** UNSPECIFIED - Standard machine type. */
  UNSPECIFIED = 0,
  /** N1_HIGHCPU_8 - Highcpu machine with 8 CPUs. */
  N1_HIGHCPU_8 = 1,
  /** N1_HIGHCPU_32 - Highcpu machine with 32 CPUs. */
  N1_HIGHCPU_32 = 2,
  UNRECOGNIZED = -1,
}

export function buildOptions_MachineTypeFromJSON(object: any): BuildOptions_MachineType {
  switch (object) {
    case 0:
    case "UNSPECIFIED":
      return BuildOptions_MachineType.UNSPECIFIED;
    case 1:
    case "N1_HIGHCPU_8":
      return BuildOptions_MachineType.N1_HIGHCPU_8;
    case 2:
    case "N1_HIGHCPU_32":
      return BuildOptions_MachineType.N1_HIGHCPU_32;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BuildOptions_MachineType.UNRECOGNIZED;
  }
}

export function buildOptions_MachineTypeToJSON(object: BuildOptions_MachineType): string {
  switch (object) {
    case BuildOptions_MachineType.UNSPECIFIED:
      return "UNSPECIFIED";
    case BuildOptions_MachineType.N1_HIGHCPU_8:
      return "N1_HIGHCPU_8";
    case BuildOptions_MachineType.N1_HIGHCPU_32:
      return "N1_HIGHCPU_32";
    case BuildOptions_MachineType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies the behavior when there is an error in the substitution checks. */
export enum BuildOptions_SubstitutionOption {
  /**
   * MUST_MATCH - Fails the build if error in substitutions checks, like missing
   * a substitution in the template or in the map.
   */
  MUST_MATCH = 0,
  /** ALLOW_LOOSE - Do not fail the build if error in substitutions checks. */
  ALLOW_LOOSE = 1,
  UNRECOGNIZED = -1,
}

export function buildOptions_SubstitutionOptionFromJSON(object: any): BuildOptions_SubstitutionOption {
  switch (object) {
    case 0:
    case "MUST_MATCH":
      return BuildOptions_SubstitutionOption.MUST_MATCH;
    case 1:
    case "ALLOW_LOOSE":
      return BuildOptions_SubstitutionOption.ALLOW_LOOSE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BuildOptions_SubstitutionOption.UNRECOGNIZED;
  }
}

export function buildOptions_SubstitutionOptionToJSON(object: BuildOptions_SubstitutionOption): string {
  switch (object) {
    case BuildOptions_SubstitutionOption.MUST_MATCH:
      return "MUST_MATCH";
    case BuildOptions_SubstitutionOption.ALLOW_LOOSE:
      return "ALLOW_LOOSE";
    case BuildOptions_SubstitutionOption.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies the behavior when writing build logs to Google Cloud Storage. */
export enum BuildOptions_LogStreamingOption {
  /** STREAM_DEFAULT - Service may automatically determine build log streaming behavior. */
  STREAM_DEFAULT = 0,
  /** STREAM_ON - Build logs should be streamed to Google Cloud Storage. */
  STREAM_ON = 1,
  /**
   * STREAM_OFF - Build logs should not be streamed to Google Cloud Storage; they will be
   * written when the build is completed.
   */
  STREAM_OFF = 2,
  UNRECOGNIZED = -1,
}

export function buildOptions_LogStreamingOptionFromJSON(object: any): BuildOptions_LogStreamingOption {
  switch (object) {
    case 0:
    case "STREAM_DEFAULT":
      return BuildOptions_LogStreamingOption.STREAM_DEFAULT;
    case 1:
    case "STREAM_ON":
      return BuildOptions_LogStreamingOption.STREAM_ON;
    case 2:
    case "STREAM_OFF":
      return BuildOptions_LogStreamingOption.STREAM_OFF;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BuildOptions_LogStreamingOption.UNRECOGNIZED;
  }
}

export function buildOptions_LogStreamingOptionToJSON(object: BuildOptions_LogStreamingOption): string {
  switch (object) {
    case BuildOptions_LogStreamingOption.STREAM_DEFAULT:
      return "STREAM_DEFAULT";
    case BuildOptions_LogStreamingOption.STREAM_ON:
      return "STREAM_ON";
    case BuildOptions_LogStreamingOption.STREAM_OFF:
      return "STREAM_OFF";
    case BuildOptions_LogStreamingOption.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies the logging mode. */
export enum BuildOptions_LoggingMode {
  /**
   * LOGGING_UNSPECIFIED - The service determines the logging mode. The default is `LEGACY`. Do not
   * rely on the default logging behavior as it may change in the future.
   */
  LOGGING_UNSPECIFIED = 0,
  /** LEGACY - Stackdriver logging and Cloud Storage logging are enabled. */
  LEGACY = 1,
  /** GCS_ONLY - Only Cloud Storage logging is enabled. */
  GCS_ONLY = 2,
  UNRECOGNIZED = -1,
}

export function buildOptions_LoggingModeFromJSON(object: any): BuildOptions_LoggingMode {
  switch (object) {
    case 0:
    case "LOGGING_UNSPECIFIED":
      return BuildOptions_LoggingMode.LOGGING_UNSPECIFIED;
    case 1:
    case "LEGACY":
      return BuildOptions_LoggingMode.LEGACY;
    case 2:
    case "GCS_ONLY":
      return BuildOptions_LoggingMode.GCS_ONLY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BuildOptions_LoggingMode.UNRECOGNIZED;
  }
}

export function buildOptions_LoggingModeToJSON(object: BuildOptions_LoggingMode): string {
  switch (object) {
    case BuildOptions_LoggingMode.LOGGING_UNSPECIFIED:
      return "LOGGING_UNSPECIFIED";
    case BuildOptions_LoggingMode.LEGACY:
      return "LEGACY";
    case BuildOptions_LoggingMode.GCS_ONLY:
      return "GCS_ONLY";
    case BuildOptions_LoggingMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseBuildEventData(): BuildEventData {
  return {
    id: "",
    projectId: "",
    status: 0,
    statusDetail: "",
    source: undefined,
    steps: [],
    results: undefined,
    createTime: undefined,
    startTime: undefined,
    finishTime: undefined,
    timeout: undefined,
    images: [],
    queueTtl: undefined,
    artifacts: undefined,
    logsBucket: "",
    sourceProvenance: undefined,
    buildTriggerId: "",
    options: undefined,
    logUrl: "",
    substitutions: {},
    tags: [],
    secrets: [],
    timing: {},
  };
}

export const BuildEventData: MessageFns<BuildEventData> = {
  encode(message: BuildEventData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    if (message.projectId !== "") {
      writer.uint32(130).string(message.projectId);
    }
    if (message.status !== 0) {
      writer.uint32(16).int32(message.status);
    }
    if (message.statusDetail !== "") {
      writer.uint32(194).string(message.statusDetail);
    }
    if (message.source !== undefined) {
      Source.encode(message.source, writer.uint32(26).fork()).join();
    }
    for (const v of message.steps) {
      BuildStep.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.results !== undefined) {
      Results.encode(message.results, writer.uint32(82).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(50).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(58).fork()).join();
    }
    if (message.finishTime !== undefined) {
      Timestamp.encode(toTimestamp(message.finishTime), writer.uint32(66).fork()).join();
    }
    if (message.timeout !== undefined) {
      Duration.encode(message.timeout, writer.uint32(98).fork()).join();
    }
    for (const v of message.images) {
      writer.uint32(106).string(v!);
    }
    if (message.queueTtl !== undefined) {
      Duration.encode(message.queueTtl, writer.uint32(322).fork()).join();
    }
    if (message.artifacts !== undefined) {
      Artifacts.encode(message.artifacts, writer.uint32(298).fork()).join();
    }
    if (message.logsBucket !== "") {
      writer.uint32(154).string(message.logsBucket);
    }
    if (message.sourceProvenance !== undefined) {
      SourceProvenance.encode(message.sourceProvenance, writer.uint32(170).fork()).join();
    }
    if (message.buildTriggerId !== "") {
      writer.uint32(178).string(message.buildTriggerId);
    }
    if (message.options !== undefined) {
      BuildOptions.encode(message.options, writer.uint32(186).fork()).join();
    }
    if (message.logUrl !== "") {
      writer.uint32(202).string(message.logUrl);
    }
    Object.entries(message.substitutions).forEach(([key, value]) => {
      BuildEventData_SubstitutionsEntry.encode({ key: key as any, value }, writer.uint32(234).fork()).join();
    });
    for (const v of message.tags) {
      writer.uint32(250).string(v!);
    }
    for (const v of message.secrets) {
      Secret.encode(v!, writer.uint32(258).fork()).join();
    }
    Object.entries(message.timing).forEach(([key, value]) => {
      BuildEventData_TimingEntry.encode({ key: key as any, value }, writer.uint32(266).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BuildEventData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildEventData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        }
        case 16: {
          if (tag !== 130) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        }
        case 24: {
          if (tag !== 194) {
            break;
          }

          message.statusDetail = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.source = Source.decode(reader, reader.uint32());
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.steps.push(BuildStep.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.results = Results.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.finishTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.timeout = Duration.decode(reader, reader.uint32());
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.images.push(reader.string());
          continue;
        }
        case 40: {
          if (tag !== 322) {
            break;
          }

          message.queueTtl = Duration.decode(reader, reader.uint32());
          continue;
        }
        case 37: {
          if (tag !== 298) {
            break;
          }

          message.artifacts = Artifacts.decode(reader, reader.uint32());
          continue;
        }
        case 19: {
          if (tag !== 154) {
            break;
          }

          message.logsBucket = reader.string();
          continue;
        }
        case 21: {
          if (tag !== 170) {
            break;
          }

          message.sourceProvenance = SourceProvenance.decode(reader, reader.uint32());
          continue;
        }
        case 22: {
          if (tag !== 178) {
            break;
          }

          message.buildTriggerId = reader.string();
          continue;
        }
        case 23: {
          if (tag !== 186) {
            break;
          }

          message.options = BuildOptions.decode(reader, reader.uint32());
          continue;
        }
        case 25: {
          if (tag !== 202) {
            break;
          }

          message.logUrl = reader.string();
          continue;
        }
        case 29: {
          if (tag !== 234) {
            break;
          }

          const entry29 = BuildEventData_SubstitutionsEntry.decode(reader, reader.uint32());
          if (entry29.value !== undefined) {
            message.substitutions[entry29.key] = entry29.value;
          }
          continue;
        }
        case 31: {
          if (tag !== 250) {
            break;
          }

          message.tags.push(reader.string());
          continue;
        }
        case 32: {
          if (tag !== 258) {
            break;
          }

          message.secrets.push(Secret.decode(reader, reader.uint32()));
          continue;
        }
        case 33: {
          if (tag !== 266) {
            break;
          }

          const entry33 = BuildEventData_TimingEntry.decode(reader, reader.uint32());
          if (entry33.value !== undefined) {
            message.timing[entry33.key] = entry33.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildEventData {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      status: isSet(object.status) ? buildEventData_StatusFromJSON(object.status) : 0,
      statusDetail: isSet(object.statusDetail) ? globalThis.String(object.statusDetail) : "",
      source: isSet(object.source) ? Source.fromJSON(object.source) : undefined,
      steps: globalThis.Array.isArray(object?.steps) ? object.steps.map((e: any) => BuildStep.fromJSON(e)) : [],
      results: isSet(object.results) ? Results.fromJSON(object.results) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      finishTime: isSet(object.finishTime) ? fromJsonTimestamp(object.finishTime) : undefined,
      timeout: isSet(object.timeout) ? Duration.fromJSON(object.timeout) : undefined,
      images: globalThis.Array.isArray(object?.images) ? object.images.map((e: any) => globalThis.String(e)) : [],
      queueTtl: isSet(object.queueTtl) ? Duration.fromJSON(object.queueTtl) : undefined,
      artifacts: isSet(object.artifacts) ? Artifacts.fromJSON(object.artifacts) : undefined,
      logsBucket: isSet(object.logsBucket) ? globalThis.String(object.logsBucket) : "",
      sourceProvenance: isSet(object.sourceProvenance) ? SourceProvenance.fromJSON(object.sourceProvenance) : undefined,
      buildTriggerId: isSet(object.buildTriggerId) ? globalThis.String(object.buildTriggerId) : "",
      options: isSet(object.options) ? BuildOptions.fromJSON(object.options) : undefined,
      logUrl: isSet(object.logUrl) ? globalThis.String(object.logUrl) : "",
      substitutions: isObject(object.substitutions)
        ? Object.entries(object.substitutions).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      tags: globalThis.Array.isArray(object?.tags) ? object.tags.map((e: any) => globalThis.String(e)) : [],
      secrets: globalThis.Array.isArray(object?.secrets) ? object.secrets.map((e: any) => Secret.fromJSON(e)) : [],
      timing: isObject(object.timing)
        ? Object.entries(object.timing).reduce<{ [key: string]: TimeSpan }>((acc, [key, value]) => {
          acc[key] = TimeSpan.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: BuildEventData): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.status !== 0) {
      obj.status = buildEventData_StatusToJSON(message.status);
    }
    if (message.statusDetail !== "") {
      obj.statusDetail = message.statusDetail;
    }
    if (message.source !== undefined) {
      obj.source = Source.toJSON(message.source);
    }
    if (message.steps?.length) {
      obj.steps = message.steps.map((e) => BuildStep.toJSON(e));
    }
    if (message.results !== undefined) {
      obj.results = Results.toJSON(message.results);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.finishTime !== undefined) {
      obj.finishTime = message.finishTime.toISOString();
    }
    if (message.timeout !== undefined) {
      obj.timeout = Duration.toJSON(message.timeout);
    }
    if (message.images?.length) {
      obj.images = message.images;
    }
    if (message.queueTtl !== undefined) {
      obj.queueTtl = Duration.toJSON(message.queueTtl);
    }
    if (message.artifacts !== undefined) {
      obj.artifacts = Artifacts.toJSON(message.artifacts);
    }
    if (message.logsBucket !== "") {
      obj.logsBucket = message.logsBucket;
    }
    if (message.sourceProvenance !== undefined) {
      obj.sourceProvenance = SourceProvenance.toJSON(message.sourceProvenance);
    }
    if (message.buildTriggerId !== "") {
      obj.buildTriggerId = message.buildTriggerId;
    }
    if (message.options !== undefined) {
      obj.options = BuildOptions.toJSON(message.options);
    }
    if (message.logUrl !== "") {
      obj.logUrl = message.logUrl;
    }
    if (message.substitutions) {
      const entries = Object.entries(message.substitutions);
      if (entries.length > 0) {
        obj.substitutions = {};
        entries.forEach(([k, v]) => {
          obj.substitutions[k] = v;
        });
      }
    }
    if (message.tags?.length) {
      obj.tags = message.tags;
    }
    if (message.secrets?.length) {
      obj.secrets = message.secrets.map((e) => Secret.toJSON(e));
    }
    if (message.timing) {
      const entries = Object.entries(message.timing);
      if (entries.length > 0) {
        obj.timing = {};
        entries.forEach(([k, v]) => {
          obj.timing[k] = TimeSpan.toJSON(v);
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BuildEventData>, I>>(base?: I): BuildEventData {
    return BuildEventData.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BuildEventData>, I>>(object: I): BuildEventData {
    const message = createBaseBuildEventData();
    message.id = object.id ?? "";
    message.projectId = object.projectId ?? "";
    message.status = object.status ?? 0;
    message.statusDetail = object.statusDetail ?? "";
    message.source = (object.source !== undefined && object.source !== null)
      ? Source.fromPartial(object.source)
      : undefined;
    message.steps = object.steps?.map((e) => BuildStep.fromPartial(e)) || [];
    message.results = (object.results !== undefined && object.results !== null)
      ? Results.fromPartial(object.results)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.finishTime = object.finishTime ?? undefined;
    message.timeout = (object.timeout !== undefined && object.timeout !== null)
      ? Duration.fromPartial(object.timeout)
      : undefined;
    message.images = object.images?.map((e) => e) || [];
    message.queueTtl = (object.queueTtl !== undefined && object.queueTtl !== null)
      ? Duration.fromPartial(object.queueTtl)
      : undefined;
    message.artifacts = (object.artifacts !== undefined && object.artifacts !== null)
      ? Artifacts.fromPartial(object.artifacts)
      : undefined;
    message.logsBucket = object.logsBucket ?? "";
    message.sourceProvenance = (object.sourceProvenance !== undefined && object.sourceProvenance !== null)
      ? SourceProvenance.fromPartial(object.sourceProvenance)
      : undefined;
    message.buildTriggerId = object.buildTriggerId ?? "";
    message.options = (object.options !== undefined && object.options !== null)
      ? BuildOptions.fromPartial(object.options)
      : undefined;
    message.logUrl = object.logUrl ?? "";
    message.substitutions = Object.entries(object.substitutions ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.tags = object.tags?.map((e) => e) || [];
    message.secrets = object.secrets?.map((e) => Secret.fromPartial(e)) || [];
    message.timing = Object.entries(object.timing ?? {}).reduce<{ [key: string]: TimeSpan }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = TimeSpan.fromPartial(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseBuildEventData_SubstitutionsEntry(): BuildEventData_SubstitutionsEntry {
  return { key: "", value: "" };
}

export const BuildEventData_SubstitutionsEntry: MessageFns<BuildEventData_SubstitutionsEntry> = {
  encode(message: BuildEventData_SubstitutionsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BuildEventData_SubstitutionsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildEventData_SubstitutionsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildEventData_SubstitutionsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: BuildEventData_SubstitutionsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BuildEventData_SubstitutionsEntry>, I>>(
    base?: I,
  ): BuildEventData_SubstitutionsEntry {
    return BuildEventData_SubstitutionsEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BuildEventData_SubstitutionsEntry>, I>>(
    object: I,
  ): BuildEventData_SubstitutionsEntry {
    const message = createBaseBuildEventData_SubstitutionsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseBuildEventData_TimingEntry(): BuildEventData_TimingEntry {
  return { key: "", value: undefined };
}

export const BuildEventData_TimingEntry: MessageFns<BuildEventData_TimingEntry> = {
  encode(message: BuildEventData_TimingEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      TimeSpan.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BuildEventData_TimingEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildEventData_TimingEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = TimeSpan.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildEventData_TimingEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? TimeSpan.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: BuildEventData_TimingEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = TimeSpan.toJSON(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BuildEventData_TimingEntry>, I>>(base?: I): BuildEventData_TimingEntry {
    return BuildEventData_TimingEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BuildEventData_TimingEntry>, I>>(object: I): BuildEventData_TimingEntry {
    const message = createBaseBuildEventData_TimingEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? TimeSpan.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseSource(): Source {
  return { storageSource: undefined, repoSource: undefined };
}

export const Source: MessageFns<Source> = {
  encode(message: Source, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.storageSource !== undefined) {
      StorageSource.encode(message.storageSource, writer.uint32(18).fork()).join();
    }
    if (message.repoSource !== undefined) {
      RepoSource.encode(message.repoSource, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Source {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.storageSource = StorageSource.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.repoSource = RepoSource.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Source {
    return {
      storageSource: isSet(object.storageSource) ? StorageSource.fromJSON(object.storageSource) : undefined,
      repoSource: isSet(object.repoSource) ? RepoSource.fromJSON(object.repoSource) : undefined,
    };
  },

  toJSON(message: Source): unknown {
    const obj: any = {};
    if (message.storageSource !== undefined) {
      obj.storageSource = StorageSource.toJSON(message.storageSource);
    }
    if (message.repoSource !== undefined) {
      obj.repoSource = RepoSource.toJSON(message.repoSource);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Source>, I>>(base?: I): Source {
    return Source.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Source>, I>>(object: I): Source {
    const message = createBaseSource();
    message.storageSource = (object.storageSource !== undefined && object.storageSource !== null)
      ? StorageSource.fromPartial(object.storageSource)
      : undefined;
    message.repoSource = (object.repoSource !== undefined && object.repoSource !== null)
      ? RepoSource.fromPartial(object.repoSource)
      : undefined;
    return message;
  },
};

function createBaseStorageSource(): StorageSource {
  return { bucket: "", object: "", generation: Long.ZERO };
}

export const StorageSource: MessageFns<StorageSource> = {
  encode(message: StorageSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bucket !== "") {
      writer.uint32(10).string(message.bucket);
    }
    if (message.object !== "") {
      writer.uint32(18).string(message.object);
    }
    if (!message.generation.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.generation.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StorageSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStorageSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.bucket = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.object = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.generation = Long.fromString(reader.int64().toString());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StorageSource {
    return {
      bucket: isSet(object.bucket) ? globalThis.String(object.bucket) : "",
      object: isSet(object.object) ? globalThis.String(object.object) : "",
      generation: isSet(object.generation) ? Long.fromValue(object.generation) : Long.ZERO,
    };
  },

  toJSON(message: StorageSource): unknown {
    const obj: any = {};
    if (message.bucket !== "") {
      obj.bucket = message.bucket;
    }
    if (message.object !== "") {
      obj.object = message.object;
    }
    if (!message.generation.equals(Long.ZERO)) {
      obj.generation = (message.generation || Long.ZERO).toString();
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StorageSource>, I>>(base?: I): StorageSource {
    return StorageSource.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StorageSource>, I>>(object: I): StorageSource {
    const message = createBaseStorageSource();
    message.bucket = object.bucket ?? "";
    message.object = object.object ?? "";
    message.generation = (object.generation !== undefined && object.generation !== null)
      ? Long.fromValue(object.generation)
      : Long.ZERO;
    return message;
  },
};

function createBaseRepoSource(): RepoSource {
  return {
    projectId: "",
    repoName: "",
    branchName: undefined,
    tagName: undefined,
    commitSha: undefined,
    dir: "",
    invertRegex: false,
    substitutions: {},
  };
}

export const RepoSource: MessageFns<RepoSource> = {
  encode(message: RepoSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.repoName !== "") {
      writer.uint32(18).string(message.repoName);
    }
    if (message.branchName !== undefined) {
      writer.uint32(26).string(message.branchName);
    }
    if (message.tagName !== undefined) {
      writer.uint32(34).string(message.tagName);
    }
    if (message.commitSha !== undefined) {
      writer.uint32(42).string(message.commitSha);
    }
    if (message.dir !== "") {
      writer.uint32(58).string(message.dir);
    }
    if (message.invertRegex !== false) {
      writer.uint32(64).bool(message.invertRegex);
    }
    Object.entries(message.substitutions).forEach(([key, value]) => {
      RepoSource_SubstitutionsEntry.encode({ key: key as any, value }, writer.uint32(74).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RepoSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRepoSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.repoName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.branchName = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.tagName = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.commitSha = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.dir = reader.string();
          continue;
        }
        case 8: {
          if (tag !== 64) {
            break;
          }

          message.invertRegex = reader.bool();
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          const entry9 = RepoSource_SubstitutionsEntry.decode(reader, reader.uint32());
          if (entry9.value !== undefined) {
            message.substitutions[entry9.key] = entry9.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RepoSource {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      repoName: isSet(object.repoName) ? globalThis.String(object.repoName) : "",
      branchName: isSet(object.branchName) ? globalThis.String(object.branchName) : undefined,
      tagName: isSet(object.tagName) ? globalThis.String(object.tagName) : undefined,
      commitSha: isSet(object.commitSha) ? globalThis.String(object.commitSha) : undefined,
      dir: isSet(object.dir) ? globalThis.String(object.dir) : "",
      invertRegex: isSet(object.invertRegex) ? globalThis.Boolean(object.invertRegex) : false,
      substitutions: isObject(object.substitutions)
        ? Object.entries(object.substitutions).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: RepoSource): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.repoName !== "") {
      obj.repoName = message.repoName;
    }
    if (message.branchName !== undefined) {
      obj.branchName = message.branchName;
    }
    if (message.tagName !== undefined) {
      obj.tagName = message.tagName;
    }
    if (message.commitSha !== undefined) {
      obj.commitSha = message.commitSha;
    }
    if (message.dir !== "") {
      obj.dir = message.dir;
    }
    if (message.invertRegex !== false) {
      obj.invertRegex = message.invertRegex;
    }
    if (message.substitutions) {
      const entries = Object.entries(message.substitutions);
      if (entries.length > 0) {
        obj.substitutions = {};
        entries.forEach(([k, v]) => {
          obj.substitutions[k] = v;
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RepoSource>, I>>(base?: I): RepoSource {
    return RepoSource.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RepoSource>, I>>(object: I): RepoSource {
    const message = createBaseRepoSource();
    message.projectId = object.projectId ?? "";
    message.repoName = object.repoName ?? "";
    message.branchName = object.branchName ?? undefined;
    message.tagName = object.tagName ?? undefined;
    message.commitSha = object.commitSha ?? undefined;
    message.dir = object.dir ?? "";
    message.invertRegex = object.invertRegex ?? false;
    message.substitutions = Object.entries(object.substitutions ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseRepoSource_SubstitutionsEntry(): RepoSource_SubstitutionsEntry {
  return { key: "", value: "" };
}

export const RepoSource_SubstitutionsEntry: MessageFns<RepoSource_SubstitutionsEntry> = {
  encode(message: RepoSource_SubstitutionsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RepoSource_SubstitutionsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRepoSource_SubstitutionsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RepoSource_SubstitutionsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RepoSource_SubstitutionsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RepoSource_SubstitutionsEntry>, I>>(base?: I): RepoSource_SubstitutionsEntry {
    return RepoSource_SubstitutionsEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RepoSource_SubstitutionsEntry>, I>>(
    object: I,
  ): RepoSource_SubstitutionsEntry {
    const message = createBaseRepoSource_SubstitutionsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseBuildStep(): BuildStep {
  return {
    name: "",
    env: [],
    args: [],
    dir: "",
    id: "",
    waitFor: [],
    entrypoint: "",
    secretEnv: [],
    volumes: [],
    timing: undefined,
    pullTiming: undefined,
    timeout: undefined,
    status: 0,
  };
}

export const BuildStep: MessageFns<BuildStep> = {
  encode(message: BuildStep, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    for (const v of message.env) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.args) {
      writer.uint32(26).string(v!);
    }
    if (message.dir !== "") {
      writer.uint32(34).string(message.dir);
    }
    if (message.id !== "") {
      writer.uint32(42).string(message.id);
    }
    for (const v of message.waitFor) {
      writer.uint32(50).string(v!);
    }
    if (message.entrypoint !== "") {
      writer.uint32(58).string(message.entrypoint);
    }
    for (const v of message.secretEnv) {
      writer.uint32(66).string(v!);
    }
    for (const v of message.volumes) {
      Volume.encode(v!, writer.uint32(74).fork()).join();
    }
    if (message.timing !== undefined) {
      TimeSpan.encode(message.timing, writer.uint32(82).fork()).join();
    }
    if (message.pullTiming !== undefined) {
      TimeSpan.encode(message.pullTiming, writer.uint32(106).fork()).join();
    }
    if (message.timeout !== undefined) {
      Duration.encode(message.timeout, writer.uint32(90).fork()).join();
    }
    if (message.status !== 0) {
      writer.uint32(96).int32(message.status);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BuildStep {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildStep();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.env.push(reader.string());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.args.push(reader.string());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.dir = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.id = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.waitFor.push(reader.string());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.entrypoint = reader.string();
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.secretEnv.push(reader.string());
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.volumes.push(Volume.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.timing = TimeSpan.decode(reader, reader.uint32());
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.pullTiming = TimeSpan.decode(reader, reader.uint32());
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.timeout = Duration.decode(reader, reader.uint32());
          continue;
        }
        case 12: {
          if (tag !== 96) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildStep {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      env: globalThis.Array.isArray(object?.env) ? object.env.map((e: any) => globalThis.String(e)) : [],
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      dir: isSet(object.dir) ? globalThis.String(object.dir) : "",
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      waitFor: globalThis.Array.isArray(object?.waitFor) ? object.waitFor.map((e: any) => globalThis.String(e)) : [],
      entrypoint: isSet(object.entrypoint) ? globalThis.String(object.entrypoint) : "",
      secretEnv: globalThis.Array.isArray(object?.secretEnv)
        ? object.secretEnv.map((e: any) => globalThis.String(e))
        : [],
      volumes: globalThis.Array.isArray(object?.volumes) ? object.volumes.map((e: any) => Volume.fromJSON(e)) : [],
      timing: isSet(object.timing) ? TimeSpan.fromJSON(object.timing) : undefined,
      pullTiming: isSet(object.pullTiming) ? TimeSpan.fromJSON(object.pullTiming) : undefined,
      timeout: isSet(object.timeout) ? Duration.fromJSON(object.timeout) : undefined,
      status: isSet(object.status) ? buildEventData_StatusFromJSON(object.status) : 0,
    };
  },

  toJSON(message: BuildStep): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.env?.length) {
      obj.env = message.env;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.dir !== "") {
      obj.dir = message.dir;
    }
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.waitFor?.length) {
      obj.waitFor = message.waitFor;
    }
    if (message.entrypoint !== "") {
      obj.entrypoint = message.entrypoint;
    }
    if (message.secretEnv?.length) {
      obj.secretEnv = message.secretEnv;
    }
    if (message.volumes?.length) {
      obj.volumes = message.volumes.map((e) => Volume.toJSON(e));
    }
    if (message.timing !== undefined) {
      obj.timing = TimeSpan.toJSON(message.timing);
    }
    if (message.pullTiming !== undefined) {
      obj.pullTiming = TimeSpan.toJSON(message.pullTiming);
    }
    if (message.timeout !== undefined) {
      obj.timeout = Duration.toJSON(message.timeout);
    }
    if (message.status !== 0) {
      obj.status = buildEventData_StatusToJSON(message.status);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BuildStep>, I>>(base?: I): BuildStep {
    return BuildStep.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BuildStep>, I>>(object: I): BuildStep {
    const message = createBaseBuildStep();
    message.name = object.name ?? "";
    message.env = object.env?.map((e) => e) || [];
    message.args = object.args?.map((e) => e) || [];
    message.dir = object.dir ?? "";
    message.id = object.id ?? "";
    message.waitFor = object.waitFor?.map((e) => e) || [];
    message.entrypoint = object.entrypoint ?? "";
    message.secretEnv = object.secretEnv?.map((e) => e) || [];
    message.volumes = object.volumes?.map((e) => Volume.fromPartial(e)) || [];
    message.timing = (object.timing !== undefined && object.timing !== null)
      ? TimeSpan.fromPartial(object.timing)
      : undefined;
    message.pullTiming = (object.pullTiming !== undefined && object.pullTiming !== null)
      ? TimeSpan.fromPartial(object.pullTiming)
      : undefined;
    message.timeout = (object.timeout !== undefined && object.timeout !== null)
      ? Duration.fromPartial(object.timeout)
      : undefined;
    message.status = object.status ?? 0;
    return message;
  },
};

function createBaseVolume(): Volume {
  return { name: "", path: "" };
}

export const Volume: MessageFns<Volume> = {
  encode(message: Volume, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.path !== "") {
      writer.uint32(18).string(message.path);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Volume {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVolume();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.path = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Volume {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      path: isSet(object.path) ? globalThis.String(object.path) : "",
    };
  },

  toJSON(message: Volume): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.path !== "") {
      obj.path = message.path;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Volume>, I>>(base?: I): Volume {
    return Volume.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Volume>, I>>(object: I): Volume {
    const message = createBaseVolume();
    message.name = object.name ?? "";
    message.path = object.path ?? "";
    return message;
  },
};

function createBaseResults(): Results {
  return {
    images: [],
    buildStepImages: [],
    artifactManifest: "",
    numArtifacts: Long.ZERO,
    buildStepOutputs: [],
    artifactTiming: undefined,
  };
}

export const Results: MessageFns<Results> = {
  encode(message: Results, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.images) {
      BuiltImage.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.buildStepImages) {
      writer.uint32(26).string(v!);
    }
    if (message.artifactManifest !== "") {
      writer.uint32(34).string(message.artifactManifest);
    }
    if (!message.numArtifacts.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.numArtifacts.toString());
    }
    for (const v of message.buildStepOutputs) {
      writer.uint32(50).bytes(v!);
    }
    if (message.artifactTiming !== undefined) {
      TimeSpan.encode(message.artifactTiming, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Results {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResults();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.images.push(BuiltImage.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.buildStepImages.push(reader.string());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.artifactManifest = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.numArtifacts = Long.fromString(reader.int64().toString());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.buildStepOutputs.push(reader.bytes());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.artifactTiming = TimeSpan.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Results {
    return {
      images: globalThis.Array.isArray(object?.images) ? object.images.map((e: any) => BuiltImage.fromJSON(e)) : [],
      buildStepImages: globalThis.Array.isArray(object?.buildStepImages)
        ? object.buildStepImages.map((e: any) => globalThis.String(e))
        : [],
      artifactManifest: isSet(object.artifactManifest) ? globalThis.String(object.artifactManifest) : "",
      numArtifacts: isSet(object.numArtifacts) ? Long.fromValue(object.numArtifacts) : Long.ZERO,
      buildStepOutputs: globalThis.Array.isArray(object?.buildStepOutputs)
        ? object.buildStepOutputs.map((e: any) => bytesFromBase64(e))
        : [],
      artifactTiming: isSet(object.artifactTiming) ? TimeSpan.fromJSON(object.artifactTiming) : undefined,
    };
  },

  toJSON(message: Results): unknown {
    const obj: any = {};
    if (message.images?.length) {
      obj.images = message.images.map((e) => BuiltImage.toJSON(e));
    }
    if (message.buildStepImages?.length) {
      obj.buildStepImages = message.buildStepImages;
    }
    if (message.artifactManifest !== "") {
      obj.artifactManifest = message.artifactManifest;
    }
    if (!message.numArtifacts.equals(Long.ZERO)) {
      obj.numArtifacts = (message.numArtifacts || Long.ZERO).toString();
    }
    if (message.buildStepOutputs?.length) {
      obj.buildStepOutputs = message.buildStepOutputs.map((e) => base64FromBytes(e));
    }
    if (message.artifactTiming !== undefined) {
      obj.artifactTiming = TimeSpan.toJSON(message.artifactTiming);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Results>, I>>(base?: I): Results {
    return Results.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Results>, I>>(object: I): Results {
    const message = createBaseResults();
    message.images = object.images?.map((e) => BuiltImage.fromPartial(e)) || [];
    message.buildStepImages = object.buildStepImages?.map((e) => e) || [];
    message.artifactManifest = object.artifactManifest ?? "";
    message.numArtifacts = (object.numArtifacts !== undefined && object.numArtifacts !== null)
      ? Long.fromValue(object.numArtifacts)
      : Long.ZERO;
    message.buildStepOutputs = object.buildStepOutputs?.map((e) => e) || [];
    message.artifactTiming = (object.artifactTiming !== undefined && object.artifactTiming !== null)
      ? TimeSpan.fromPartial(object.artifactTiming)
      : undefined;
    return message;
  },
};

function createBaseBuiltImage(): BuiltImage {
  return { name: "", digest: "", pushTiming: undefined };
}

export const BuiltImage: MessageFns<BuiltImage> = {
  encode(message: BuiltImage, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.digest !== "") {
      writer.uint32(26).string(message.digest);
    }
    if (message.pushTiming !== undefined) {
      TimeSpan.encode(message.pushTiming, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BuiltImage {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuiltImage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.digest = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.pushTiming = TimeSpan.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuiltImage {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      digest: isSet(object.digest) ? globalThis.String(object.digest) : "",
      pushTiming: isSet(object.pushTiming) ? TimeSpan.fromJSON(object.pushTiming) : undefined,
    };
  },

  toJSON(message: BuiltImage): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.digest !== "") {
      obj.digest = message.digest;
    }
    if (message.pushTiming !== undefined) {
      obj.pushTiming = TimeSpan.toJSON(message.pushTiming);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BuiltImage>, I>>(base?: I): BuiltImage {
    return BuiltImage.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BuiltImage>, I>>(object: I): BuiltImage {
    const message = createBaseBuiltImage();
    message.name = object.name ?? "";
    message.digest = object.digest ?? "";
    message.pushTiming = (object.pushTiming !== undefined && object.pushTiming !== null)
      ? TimeSpan.fromPartial(object.pushTiming)
      : undefined;
    return message;
  },
};

function createBaseArtifacts(): Artifacts {
  return { images: [], objects: undefined };
}

export const Artifacts: MessageFns<Artifacts> = {
  encode(message: Artifacts, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.images) {
      writer.uint32(10).string(v!);
    }
    if (message.objects !== undefined) {
      Artifacts_ArtifactObjects.encode(message.objects, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Artifacts {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseArtifacts();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.images.push(reader.string());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.objects = Artifacts_ArtifactObjects.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Artifacts {
    return {
      images: globalThis.Array.isArray(object?.images) ? object.images.map((e: any) => globalThis.String(e)) : [],
      objects: isSet(object.objects) ? Artifacts_ArtifactObjects.fromJSON(object.objects) : undefined,
    };
  },

  toJSON(message: Artifacts): unknown {
    const obj: any = {};
    if (message.images?.length) {
      obj.images = message.images;
    }
    if (message.objects !== undefined) {
      obj.objects = Artifacts_ArtifactObjects.toJSON(message.objects);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Artifacts>, I>>(base?: I): Artifacts {
    return Artifacts.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Artifacts>, I>>(object: I): Artifacts {
    const message = createBaseArtifacts();
    message.images = object.images?.map((e) => e) || [];
    message.objects = (object.objects !== undefined && object.objects !== null)
      ? Artifacts_ArtifactObjects.fromPartial(object.objects)
      : undefined;
    return message;
  },
};

function createBaseArtifacts_ArtifactObjects(): Artifacts_ArtifactObjects {
  return { location: "", paths: [], timing: undefined };
}

export const Artifacts_ArtifactObjects: MessageFns<Artifacts_ArtifactObjects> = {
  encode(message: Artifacts_ArtifactObjects, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.location !== "") {
      writer.uint32(10).string(message.location);
    }
    for (const v of message.paths) {
      writer.uint32(18).string(v!);
    }
    if (message.timing !== undefined) {
      TimeSpan.encode(message.timing, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Artifacts_ArtifactObjects {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseArtifacts_ArtifactObjects();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.location = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.paths.push(reader.string());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.timing = TimeSpan.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Artifacts_ArtifactObjects {
    return {
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      paths: globalThis.Array.isArray(object?.paths) ? object.paths.map((e: any) => globalThis.String(e)) : [],
      timing: isSet(object.timing) ? TimeSpan.fromJSON(object.timing) : undefined,
    };
  },

  toJSON(message: Artifacts_ArtifactObjects): unknown {
    const obj: any = {};
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.paths?.length) {
      obj.paths = message.paths;
    }
    if (message.timing !== undefined) {
      obj.timing = TimeSpan.toJSON(message.timing);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Artifacts_ArtifactObjects>, I>>(base?: I): Artifacts_ArtifactObjects {
    return Artifacts_ArtifactObjects.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Artifacts_ArtifactObjects>, I>>(object: I): Artifacts_ArtifactObjects {
    const message = createBaseArtifacts_ArtifactObjects();
    message.location = object.location ?? "";
    message.paths = object.paths?.map((e) => e) || [];
    message.timing = (object.timing !== undefined && object.timing !== null)
      ? TimeSpan.fromPartial(object.timing)
      : undefined;
    return message;
  },
};

function createBaseTimeSpan(): TimeSpan {
  return { startTime: undefined, endTime: undefined };
}

export const TimeSpan: MessageFns<TimeSpan> = {
  encode(message: TimeSpan, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(10).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimeSpan {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimeSpan();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimeSpan {
    return {
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
    };
  },

  toJSON(message: TimeSpan): unknown {
    const obj: any = {};
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TimeSpan>, I>>(base?: I): TimeSpan {
    return TimeSpan.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TimeSpan>, I>>(object: I): TimeSpan {
    const message = createBaseTimeSpan();
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    return message;
  },
};

function createBaseSourceProvenance(): SourceProvenance {
  return { resolvedStorageSource: undefined, resolvedRepoSource: undefined, fileHashes: {} };
}

export const SourceProvenance: MessageFns<SourceProvenance> = {
  encode(message: SourceProvenance, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.resolvedStorageSource !== undefined) {
      StorageSource.encode(message.resolvedStorageSource, writer.uint32(26).fork()).join();
    }
    if (message.resolvedRepoSource !== undefined) {
      RepoSource.encode(message.resolvedRepoSource, writer.uint32(50).fork()).join();
    }
    Object.entries(message.fileHashes).forEach(([key, value]) => {
      SourceProvenance_FileHashesEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SourceProvenance {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSourceProvenance();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.resolvedStorageSource = StorageSource.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.resolvedRepoSource = RepoSource.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          const entry4 = SourceProvenance_FileHashesEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.fileHashes[entry4.key] = entry4.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SourceProvenance {
    return {
      resolvedStorageSource: isSet(object.resolvedStorageSource)
        ? StorageSource.fromJSON(object.resolvedStorageSource)
        : undefined,
      resolvedRepoSource: isSet(object.resolvedRepoSource) ? RepoSource.fromJSON(object.resolvedRepoSource) : undefined,
      fileHashes: isObject(object.fileHashes)
        ? Object.entries(object.fileHashes).reduce<{ [key: string]: FileHashes }>((acc, [key, value]) => {
          acc[key] = FileHashes.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: SourceProvenance): unknown {
    const obj: any = {};
    if (message.resolvedStorageSource !== undefined) {
      obj.resolvedStorageSource = StorageSource.toJSON(message.resolvedStorageSource);
    }
    if (message.resolvedRepoSource !== undefined) {
      obj.resolvedRepoSource = RepoSource.toJSON(message.resolvedRepoSource);
    }
    if (message.fileHashes) {
      const entries = Object.entries(message.fileHashes);
      if (entries.length > 0) {
        obj.fileHashes = {};
        entries.forEach(([k, v]) => {
          obj.fileHashes[k] = FileHashes.toJSON(v);
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SourceProvenance>, I>>(base?: I): SourceProvenance {
    return SourceProvenance.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SourceProvenance>, I>>(object: I): SourceProvenance {
    const message = createBaseSourceProvenance();
    message.resolvedStorageSource =
      (object.resolvedStorageSource !== undefined && object.resolvedStorageSource !== null)
        ? StorageSource.fromPartial(object.resolvedStorageSource)
        : undefined;
    message.resolvedRepoSource = (object.resolvedRepoSource !== undefined && object.resolvedRepoSource !== null)
      ? RepoSource.fromPartial(object.resolvedRepoSource)
      : undefined;
    message.fileHashes = Object.entries(object.fileHashes ?? {}).reduce<{ [key: string]: FileHashes }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = FileHashes.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseSourceProvenance_FileHashesEntry(): SourceProvenance_FileHashesEntry {
  return { key: "", value: undefined };
}

export const SourceProvenance_FileHashesEntry: MessageFns<SourceProvenance_FileHashesEntry> = {
  encode(message: SourceProvenance_FileHashesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      FileHashes.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SourceProvenance_FileHashesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSourceProvenance_FileHashesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = FileHashes.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SourceProvenance_FileHashesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? FileHashes.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: SourceProvenance_FileHashesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = FileHashes.toJSON(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SourceProvenance_FileHashesEntry>, I>>(
    base?: I,
  ): SourceProvenance_FileHashesEntry {
    return SourceProvenance_FileHashesEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SourceProvenance_FileHashesEntry>, I>>(
    object: I,
  ): SourceProvenance_FileHashesEntry {
    const message = createBaseSourceProvenance_FileHashesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? FileHashes.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseFileHashes(): FileHashes {
  return { fileHash: [] };
}

export const FileHashes: MessageFns<FileHashes> = {
  encode(message: FileHashes, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.fileHash) {
      Hash.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FileHashes {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFileHashes();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.fileHash.push(Hash.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FileHashes {
    return {
      fileHash: globalThis.Array.isArray(object?.fileHash) ? object.fileHash.map((e: any) => Hash.fromJSON(e)) : [],
    };
  },

  toJSON(message: FileHashes): unknown {
    const obj: any = {};
    if (message.fileHash?.length) {
      obj.fileHash = message.fileHash.map((e) => Hash.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<FileHashes>, I>>(base?: I): FileHashes {
    return FileHashes.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<FileHashes>, I>>(object: I): FileHashes {
    const message = createBaseFileHashes();
    message.fileHash = object.fileHash?.map((e) => Hash.fromPartial(e)) || [];
    return message;
  },
};

function createBaseHash(): Hash {
  return { type: 0, value: new Uint8Array(0) };
}

export const Hash: MessageFns<Hash> = {
  encode(message: Hash, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.value.length !== 0) {
      writer.uint32(18).bytes(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Hash {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHash();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.bytes();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Hash {
    return {
      type: isSet(object.type) ? hash_HashTypeFromJSON(object.type) : 0,
      value: isSet(object.value) ? bytesFromBase64(object.value) : new Uint8Array(0),
    };
  },

  toJSON(message: Hash): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = hash_HashTypeToJSON(message.type);
    }
    if (message.value.length !== 0) {
      obj.value = base64FromBytes(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Hash>, I>>(base?: I): Hash {
    return Hash.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Hash>, I>>(object: I): Hash {
    const message = createBaseHash();
    message.type = object.type ?? 0;
    message.value = object.value ?? new Uint8Array(0);
    return message;
  },
};

function createBaseSecret(): Secret {
  return { kmsKeyName: "", secretEnv: {} };
}

export const Secret: MessageFns<Secret> = {
  encode(message: Secret, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyName !== "") {
      writer.uint32(10).string(message.kmsKeyName);
    }
    Object.entries(message.secretEnv).forEach(([key, value]) => {
      Secret_SecretEnvEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Secret {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSecret();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          const entry3 = Secret_SecretEnvEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.secretEnv[entry3.key] = entry3.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Secret {
    return {
      kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "",
      secretEnv: isObject(object.secretEnv)
        ? Object.entries(object.secretEnv).reduce<{ [key: string]: Uint8Array }>((acc, [key, value]) => {
          acc[key] = bytesFromBase64(value as string);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: Secret): unknown {
    const obj: any = {};
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    if (message.secretEnv) {
      const entries = Object.entries(message.secretEnv);
      if (entries.length > 0) {
        obj.secretEnv = {};
        entries.forEach(([k, v]) => {
          obj.secretEnv[k] = base64FromBytes(v);
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Secret>, I>>(base?: I): Secret {
    return Secret.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Secret>, I>>(object: I): Secret {
    const message = createBaseSecret();
    message.kmsKeyName = object.kmsKeyName ?? "";
    message.secretEnv = Object.entries(object.secretEnv ?? {}).reduce<{ [key: string]: Uint8Array }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = value;
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseSecret_SecretEnvEntry(): Secret_SecretEnvEntry {
  return { key: "", value: new Uint8Array(0) };
}

export const Secret_SecretEnvEntry: MessageFns<Secret_SecretEnvEntry> = {
  encode(message: Secret_SecretEnvEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value.length !== 0) {
      writer.uint32(18).bytes(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Secret_SecretEnvEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSecret_SecretEnvEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.bytes();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Secret_SecretEnvEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? bytesFromBase64(object.value) : new Uint8Array(0),
    };
  },

  toJSON(message: Secret_SecretEnvEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value.length !== 0) {
      obj.value = base64FromBytes(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Secret_SecretEnvEntry>, I>>(base?: I): Secret_SecretEnvEntry {
    return Secret_SecretEnvEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Secret_SecretEnvEntry>, I>>(object: I): Secret_SecretEnvEntry {
    const message = createBaseSecret_SecretEnvEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? new Uint8Array(0);
    return message;
  },
};

function createBaseBuildOptions(): BuildOptions {
  return {
    sourceProvenanceHash: [],
    requestedVerifyOption: 0,
    machineType: 0,
    diskSizeGb: Long.ZERO,
    substitutionOption: 0,
    logStreamingOption: 0,
    workerPool: "",
    logging: 0,
    env: [],
    secretEnv: [],
    volumes: [],
  };
}

export const BuildOptions: MessageFns<BuildOptions> = {
  encode(message: BuildOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.sourceProvenanceHash) {
      writer.int32(v);
    }
    writer.join();
    if (message.requestedVerifyOption !== 0) {
      writer.uint32(16).int32(message.requestedVerifyOption);
    }
    if (message.machineType !== 0) {
      writer.uint32(24).int32(message.machineType);
    }
    if (!message.diskSizeGb.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.diskSizeGb.toString());
    }
    if (message.substitutionOption !== 0) {
      writer.uint32(32).int32(message.substitutionOption);
    }
    if (message.logStreamingOption !== 0) {
      writer.uint32(40).int32(message.logStreamingOption);
    }
    if (message.workerPool !== "") {
      writer.uint32(58).string(message.workerPool);
    }
    if (message.logging !== 0) {
      writer.uint32(88).int32(message.logging);
    }
    for (const v of message.env) {
      writer.uint32(98).string(v!);
    }
    for (const v of message.secretEnv) {
      writer.uint32(106).string(v!);
    }
    for (const v of message.volumes) {
      Volume.encode(v!, writer.uint32(114).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BuildOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag === 8) {
            message.sourceProvenanceHash.push(reader.int32() as any);

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.sourceProvenanceHash.push(reader.int32() as any);
            }

            continue;
          }

          break;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.requestedVerifyOption = reader.int32() as any;
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.machineType = reader.int32() as any;
          continue;
        }
        case 6: {
          if (tag !== 48) {
            break;
          }

          message.diskSizeGb = Long.fromString(reader.int64().toString());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.substitutionOption = reader.int32() as any;
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.logStreamingOption = reader.int32() as any;
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.workerPool = reader.string();
          continue;
        }
        case 11: {
          if (tag !== 88) {
            break;
          }

          message.logging = reader.int32() as any;
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.env.push(reader.string());
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.secretEnv.push(reader.string());
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.volumes.push(Volume.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildOptions {
    return {
      sourceProvenanceHash: globalThis.Array.isArray(object?.sourceProvenanceHash)
        ? object.sourceProvenanceHash.map((e: any) => hash_HashTypeFromJSON(e))
        : [],
      requestedVerifyOption: isSet(object.requestedVerifyOption)
        ? buildOptions_VerifyOptionFromJSON(object.requestedVerifyOption)
        : 0,
      machineType: isSet(object.machineType) ? buildOptions_MachineTypeFromJSON(object.machineType) : 0,
      diskSizeGb: isSet(object.diskSizeGb) ? Long.fromValue(object.diskSizeGb) : Long.ZERO,
      substitutionOption: isSet(object.substitutionOption)
        ? buildOptions_SubstitutionOptionFromJSON(object.substitutionOption)
        : 0,
      logStreamingOption: isSet(object.logStreamingOption)
        ? buildOptions_LogStreamingOptionFromJSON(object.logStreamingOption)
        : 0,
      workerPool: isSet(object.workerPool) ? globalThis.String(object.workerPool) : "",
      logging: isSet(object.logging) ? buildOptions_LoggingModeFromJSON(object.logging) : 0,
      env: globalThis.Array.isArray(object?.env)
        ? object.env.map((e: any) => globalThis.String(e))
        : [],
      secretEnv: globalThis.Array.isArray(object?.secretEnv)
        ? object.secretEnv.map((e: any) => globalThis.String(e))
        : [],
      volumes: globalThis.Array.isArray(object?.volumes)
        ? object.volumes.map((e: any) => Volume.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BuildOptions): unknown {
    const obj: any = {};
    if (message.sourceProvenanceHash?.length) {
      obj.sourceProvenanceHash = message.sourceProvenanceHash.map((e) => hash_HashTypeToJSON(e));
    }
    if (message.requestedVerifyOption !== 0) {
      obj.requestedVerifyOption = buildOptions_VerifyOptionToJSON(message.requestedVerifyOption);
    }
    if (message.machineType !== 0) {
      obj.machineType = buildOptions_MachineTypeToJSON(message.machineType);
    }
    if (!message.diskSizeGb.equals(Long.ZERO)) {
      obj.diskSizeGb = (message.diskSizeGb || Long.ZERO).toString();
    }
    if (message.substitutionOption !== 0) {
      obj.substitutionOption = buildOptions_SubstitutionOptionToJSON(message.substitutionOption);
    }
    if (message.logStreamingOption !== 0) {
      obj.logStreamingOption = buildOptions_LogStreamingOptionToJSON(message.logStreamingOption);
    }
    if (message.workerPool !== "") {
      obj.workerPool = message.workerPool;
    }
    if (message.logging !== 0) {
      obj.logging = buildOptions_LoggingModeToJSON(message.logging);
    }
    if (message.env?.length) {
      obj.env = message.env;
    }
    if (message.secretEnv?.length) {
      obj.secretEnv = message.secretEnv;
    }
    if (message.volumes?.length) {
      obj.volumes = message.volumes.map((e) => Volume.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BuildOptions>, I>>(base?: I): BuildOptions {
    return BuildOptions.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BuildOptions>, I>>(object: I): BuildOptions {
    const message = createBaseBuildOptions();
    message.sourceProvenanceHash = object.sourceProvenanceHash?.map((e) => e) || [];
    message.requestedVerifyOption = object.requestedVerifyOption ?? 0;
    message.machineType = object.machineType ?? 0;
    message.diskSizeGb = (object.diskSizeGb !== undefined && object.diskSizeGb !== null)
      ? Long.fromValue(object.diskSizeGb)
      : Long.ZERO;
    message.substitutionOption = object.substitutionOption ?? 0;
    message.logStreamingOption = object.logStreamingOption ?? 0;
    message.workerPool = object.workerPool ?? "";
    message.logging = object.logging ?? 0;
    message.env = object.env?.map((e) => e) || [];
    message.secretEnv = object.secretEnv?.map((e) => e) || [];
    message.volumes = object.volumes?.map((e) => Volume.fromPartial(e)) || [];
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  if ((globalThis as any).Buffer) {
    return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
  } else {
    const bin = globalThis.atob(b64);
    const arr = new Uint8Array(bin.length);
    for (let i = 0; i < bin.length; ++i) {
      arr[i] = bin.charCodeAt(i);
    }
    return arr;
  }
}

function base64FromBytes(arr: Uint8Array): string {
  if ((globalThis as any).Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin: string[] = [];
    arr.forEach((byte) => {
      bin.push(globalThis.String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
