// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.7
//   protoc               v6.32.0
// source: google/events/cloud/dataflow/v1beta3/data.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Struct } from "../../../../protobuf/struct";
import { Timestamp } from "../../../../protobuf/timestamp";

export const protobufPackage = "google.events.cloud.dataflow.v1beta3";

/**
 * Specifies the processing model used by a
 * [google.dataflow.v1beta3.Job], which determines the way the Job is
 * managed by the Cloud Dataflow service (how workers are scheduled, how
 * inputs are sharded, etc).
 */
export enum JobType {
  /** JOB_TYPE_UNKNOWN - The type of the job is unspecified, or unknown. */
  JOB_TYPE_UNKNOWN = 0,
  /**
   * JOB_TYPE_BATCH - A batch job with a well-defined end point: data is read, data is
   * processed, data is written, and the job is done.
   */
  JOB_TYPE_BATCH = 1,
  /**
   * JOB_TYPE_STREAMING - A continuously streaming job with no end: data is read,
   * processed, and written continuously.
   */
  JOB_TYPE_STREAMING = 2,
  UNRECOGNIZED = -1,
}

export function jobTypeFromJSON(object: any): JobType {
  switch (object) {
    case 0:
    case "JOB_TYPE_UNKNOWN":
      return JobType.JOB_TYPE_UNKNOWN;
    case 1:
    case "JOB_TYPE_BATCH":
      return JobType.JOB_TYPE_BATCH;
    case 2:
    case "JOB_TYPE_STREAMING":
      return JobType.JOB_TYPE_STREAMING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobType.UNRECOGNIZED;
  }
}

export function jobTypeToJSON(object: JobType): string {
  switch (object) {
    case JobType.JOB_TYPE_UNKNOWN:
      return "JOB_TYPE_UNKNOWN";
    case JobType.JOB_TYPE_BATCH:
      return "JOB_TYPE_BATCH";
    case JobType.JOB_TYPE_STREAMING:
      return "JOB_TYPE_STREAMING";
    case JobType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies the resource to optimize for in Flexible Resource Scheduling. */
export enum FlexResourceSchedulingGoal {
  /** FLEXRS_UNSPECIFIED - Run in the default mode. */
  FLEXRS_UNSPECIFIED = 0,
  /** FLEXRS_SPEED_OPTIMIZED - Optimize for lower execution time. */
  FLEXRS_SPEED_OPTIMIZED = 1,
  /** FLEXRS_COST_OPTIMIZED - Optimize for lower cost. */
  FLEXRS_COST_OPTIMIZED = 2,
  UNRECOGNIZED = -1,
}

export function flexResourceSchedulingGoalFromJSON(object: any): FlexResourceSchedulingGoal {
  switch (object) {
    case 0:
    case "FLEXRS_UNSPECIFIED":
      return FlexResourceSchedulingGoal.FLEXRS_UNSPECIFIED;
    case 1:
    case "FLEXRS_SPEED_OPTIMIZED":
      return FlexResourceSchedulingGoal.FLEXRS_SPEED_OPTIMIZED;
    case 2:
    case "FLEXRS_COST_OPTIMIZED":
      return FlexResourceSchedulingGoal.FLEXRS_COST_OPTIMIZED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return FlexResourceSchedulingGoal.UNRECOGNIZED;
  }
}

export function flexResourceSchedulingGoalToJSON(object: FlexResourceSchedulingGoal): string {
  switch (object) {
    case FlexResourceSchedulingGoal.FLEXRS_UNSPECIFIED:
      return "FLEXRS_UNSPECIFIED";
    case FlexResourceSchedulingGoal.FLEXRS_SPEED_OPTIMIZED:
      return "FLEXRS_SPEED_OPTIMIZED";
    case FlexResourceSchedulingGoal.FLEXRS_COST_OPTIMIZED:
      return "FLEXRS_COST_OPTIMIZED";
    case FlexResourceSchedulingGoal.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies what happens to a resource when a Cloud Dataflow
 * [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job] has completed.
 */
export enum TeardownPolicy {
  /** TEARDOWN_POLICY_UNKNOWN - The teardown policy isn't specified, or is unknown. */
  TEARDOWN_POLICY_UNKNOWN = 0,
  /** TEARDOWN_ALWAYS - Always teardown the resource. */
  TEARDOWN_ALWAYS = 1,
  /**
   * TEARDOWN_ON_SUCCESS - Teardown the resource on success. This is useful for debugging
   * failures.
   */
  TEARDOWN_ON_SUCCESS = 2,
  /**
   * TEARDOWN_NEVER - Never teardown the resource. This is useful for debugging and
   * development.
   */
  TEARDOWN_NEVER = 3,
  UNRECOGNIZED = -1,
}

export function teardownPolicyFromJSON(object: any): TeardownPolicy {
  switch (object) {
    case 0:
    case "TEARDOWN_POLICY_UNKNOWN":
      return TeardownPolicy.TEARDOWN_POLICY_UNKNOWN;
    case 1:
    case "TEARDOWN_ALWAYS":
      return TeardownPolicy.TEARDOWN_ALWAYS;
    case 2:
    case "TEARDOWN_ON_SUCCESS":
      return TeardownPolicy.TEARDOWN_ON_SUCCESS;
    case 3:
    case "TEARDOWN_NEVER":
      return TeardownPolicy.TEARDOWN_NEVER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TeardownPolicy.UNRECOGNIZED;
  }
}

export function teardownPolicyToJSON(object: TeardownPolicy): string {
  switch (object) {
    case TeardownPolicy.TEARDOWN_POLICY_UNKNOWN:
      return "TEARDOWN_POLICY_UNKNOWN";
    case TeardownPolicy.TEARDOWN_ALWAYS:
      return "TEARDOWN_ALWAYS";
    case TeardownPolicy.TEARDOWN_ON_SUCCESS:
      return "TEARDOWN_ON_SUCCESS";
    case TeardownPolicy.TEARDOWN_NEVER:
      return "TEARDOWN_NEVER";
    case TeardownPolicy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The default set of packages to be staged on a pool of workers. */
export enum DefaultPackageSet {
  /** DEFAULT_PACKAGE_SET_UNKNOWN - The default set of packages to stage is unknown, or unspecified. */
  DEFAULT_PACKAGE_SET_UNKNOWN = 0,
  /**
   * DEFAULT_PACKAGE_SET_NONE - Indicates that no packages should be staged at the worker unless
   * explicitly specified by the job.
   */
  DEFAULT_PACKAGE_SET_NONE = 1,
  /** DEFAULT_PACKAGE_SET_JAVA - Stage packages typically useful to workers written in Java. */
  DEFAULT_PACKAGE_SET_JAVA = 2,
  /** DEFAULT_PACKAGE_SET_PYTHON - Stage packages typically useful to workers written in Python. */
  DEFAULT_PACKAGE_SET_PYTHON = 3,
  UNRECOGNIZED = -1,
}

export function defaultPackageSetFromJSON(object: any): DefaultPackageSet {
  switch (object) {
    case 0:
    case "DEFAULT_PACKAGE_SET_UNKNOWN":
      return DefaultPackageSet.DEFAULT_PACKAGE_SET_UNKNOWN;
    case 1:
    case "DEFAULT_PACKAGE_SET_NONE":
      return DefaultPackageSet.DEFAULT_PACKAGE_SET_NONE;
    case 2:
    case "DEFAULT_PACKAGE_SET_JAVA":
      return DefaultPackageSet.DEFAULT_PACKAGE_SET_JAVA;
    case 3:
    case "DEFAULT_PACKAGE_SET_PYTHON":
      return DefaultPackageSet.DEFAULT_PACKAGE_SET_PYTHON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DefaultPackageSet.UNRECOGNIZED;
  }
}

export function defaultPackageSetToJSON(object: DefaultPackageSet): string {
  switch (object) {
    case DefaultPackageSet.DEFAULT_PACKAGE_SET_UNKNOWN:
      return "DEFAULT_PACKAGE_SET_UNKNOWN";
    case DefaultPackageSet.DEFAULT_PACKAGE_SET_NONE:
      return "DEFAULT_PACKAGE_SET_NONE";
    case DefaultPackageSet.DEFAULT_PACKAGE_SET_JAVA:
      return "DEFAULT_PACKAGE_SET_JAVA";
    case DefaultPackageSet.DEFAULT_PACKAGE_SET_PYTHON:
      return "DEFAULT_PACKAGE_SET_PYTHON";
    case DefaultPackageSet.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies the algorithm used to determine the number of worker
 * processes to run at any given point in time, based on the amount of
 * data left to process, the number of workers, and how quickly
 * existing workers are processing data.
 */
export enum AutoscalingAlgorithm {
  /** AUTOSCALING_ALGORITHM_UNKNOWN - The algorithm is unknown, or unspecified. */
  AUTOSCALING_ALGORITHM_UNKNOWN = 0,
  /** AUTOSCALING_ALGORITHM_NONE - Disable autoscaling. */
  AUTOSCALING_ALGORITHM_NONE = 1,
  /** AUTOSCALING_ALGORITHM_BASIC - Increase worker count over time to reduce job execution time. */
  AUTOSCALING_ALGORITHM_BASIC = 2,
  UNRECOGNIZED = -1,
}

export function autoscalingAlgorithmFromJSON(object: any): AutoscalingAlgorithm {
  switch (object) {
    case 0:
    case "AUTOSCALING_ALGORITHM_UNKNOWN":
      return AutoscalingAlgorithm.AUTOSCALING_ALGORITHM_UNKNOWN;
    case 1:
    case "AUTOSCALING_ALGORITHM_NONE":
      return AutoscalingAlgorithm.AUTOSCALING_ALGORITHM_NONE;
    case 2:
    case "AUTOSCALING_ALGORITHM_BASIC":
      return AutoscalingAlgorithm.AUTOSCALING_ALGORITHM_BASIC;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AutoscalingAlgorithm.UNRECOGNIZED;
  }
}

export function autoscalingAlgorithmToJSON(object: AutoscalingAlgorithm): string {
  switch (object) {
    case AutoscalingAlgorithm.AUTOSCALING_ALGORITHM_UNKNOWN:
      return "AUTOSCALING_ALGORITHM_UNKNOWN";
    case AutoscalingAlgorithm.AUTOSCALING_ALGORITHM_NONE:
      return "AUTOSCALING_ALGORITHM_NONE";
    case AutoscalingAlgorithm.AUTOSCALING_ALGORITHM_BASIC:
      return "AUTOSCALING_ALGORITHM_BASIC";
    case AutoscalingAlgorithm.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies how IP addresses should be allocated to the worker machines. */
export enum WorkerIPAddressConfiguration {
  /** WORKER_IP_UNSPECIFIED - The configuration is unknown, or unspecified. */
  WORKER_IP_UNSPECIFIED = 0,
  /** WORKER_IP_PUBLIC - Workers should have public IP addresses. */
  WORKER_IP_PUBLIC = 1,
  /** WORKER_IP_PRIVATE - Workers should have private IP addresses. */
  WORKER_IP_PRIVATE = 2,
  UNRECOGNIZED = -1,
}

export function workerIPAddressConfigurationFromJSON(object: any): WorkerIPAddressConfiguration {
  switch (object) {
    case 0:
    case "WORKER_IP_UNSPECIFIED":
      return WorkerIPAddressConfiguration.WORKER_IP_UNSPECIFIED;
    case 1:
    case "WORKER_IP_PUBLIC":
      return WorkerIPAddressConfiguration.WORKER_IP_PUBLIC;
    case 2:
    case "WORKER_IP_PRIVATE":
      return WorkerIPAddressConfiguration.WORKER_IP_PRIVATE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return WorkerIPAddressConfiguration.UNRECOGNIZED;
  }
}

export function workerIPAddressConfigurationToJSON(object: WorkerIPAddressConfiguration): string {
  switch (object) {
    case WorkerIPAddressConfiguration.WORKER_IP_UNSPECIFIED:
      return "WORKER_IP_UNSPECIFIED";
    case WorkerIPAddressConfiguration.WORKER_IP_PUBLIC:
      return "WORKER_IP_PUBLIC";
    case WorkerIPAddressConfiguration.WORKER_IP_PRIVATE:
      return "WORKER_IP_PRIVATE";
    case WorkerIPAddressConfiguration.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies the shuffle mode used by a
 * [google.dataflow.v1beta3.Job], which determines the approach data is shuffled
 * during processing. More details in:
 * https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-shuffle
 */
export enum ShuffleMode {
  /** SHUFFLE_MODE_UNSPECIFIED - Shuffle mode information is not available. */
  SHUFFLE_MODE_UNSPECIFIED = 0,
  /** VM_BASED - Shuffle is done on the worker VMs. */
  VM_BASED = 1,
  /** SERVICE_BASED - Shuffle is done on the service side. */
  SERVICE_BASED = 2,
  UNRECOGNIZED = -1,
}

export function shuffleModeFromJSON(object: any): ShuffleMode {
  switch (object) {
    case 0:
    case "SHUFFLE_MODE_UNSPECIFIED":
      return ShuffleMode.SHUFFLE_MODE_UNSPECIFIED;
    case 1:
    case "VM_BASED":
      return ShuffleMode.VM_BASED;
    case 2:
    case "SERVICE_BASED":
      return ShuffleMode.SERVICE_BASED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ShuffleMode.UNRECOGNIZED;
  }
}

export function shuffleModeToJSON(object: ShuffleMode): string {
  switch (object) {
    case ShuffleMode.SHUFFLE_MODE_UNSPECIFIED:
      return "SHUFFLE_MODE_UNSPECIFIED";
    case ShuffleMode.VM_BASED:
      return "VM_BASED";
    case ShuffleMode.SERVICE_BASED:
      return "SERVICE_BASED";
    case ShuffleMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Describes the overall state of a
 * [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job].
 */
export enum JobState {
  /** JOB_STATE_UNKNOWN - The job's run state isn't specified. */
  JOB_STATE_UNKNOWN = 0,
  /**
   * JOB_STATE_STOPPED - `JOB_STATE_STOPPED` indicates that the job has not
   * yet started to run.
   */
  JOB_STATE_STOPPED = 1,
  /** JOB_STATE_RUNNING - `JOB_STATE_RUNNING` indicates that the job is currently running. */
  JOB_STATE_RUNNING = 2,
  /**
   * JOB_STATE_DONE - `JOB_STATE_DONE` indicates that the job has successfully completed.
   * This is a terminal job state.  This state may be set by the Cloud Dataflow
   * service, as a transition from `JOB_STATE_RUNNING`. It may also be set via a
   * Cloud Dataflow `UpdateJob` call, if the job has not yet reached a terminal
   * state.
   */
  JOB_STATE_DONE = 3,
  /**
   * JOB_STATE_FAILED - `JOB_STATE_FAILED` indicates that the job has failed.  This is a
   * terminal job state.  This state may only be set by the Cloud Dataflow
   * service, and only as a transition from `JOB_STATE_RUNNING`.
   */
  JOB_STATE_FAILED = 4,
  /**
   * JOB_STATE_CANCELLED - `JOB_STATE_CANCELLED` indicates that the job has been explicitly
   * cancelled. This is a terminal job state. This state may only be
   * set via a Cloud Dataflow `UpdateJob` call, and only if the job has not
   * yet reached another terminal state.
   */
  JOB_STATE_CANCELLED = 5,
  /**
   * JOB_STATE_UPDATED - `JOB_STATE_UPDATED` indicates that the job was successfully updated,
   * meaning that this job was stopped and another job was started, inheriting
   * state from this one. This is a terminal job state. This state may only be
   * set by the Cloud Dataflow service, and only as a transition from
   * `JOB_STATE_RUNNING`.
   */
  JOB_STATE_UPDATED = 6,
  /**
   * JOB_STATE_DRAINING - `JOB_STATE_DRAINING` indicates that the job is in the process of draining.
   * A draining job has stopped pulling from its input sources and is processing
   * any data that remains in-flight. This state may be set via a Cloud Dataflow
   * `UpdateJob` call, but only as a transition from `JOB_STATE_RUNNING`. Jobs
   * that are draining may only transition to `JOB_STATE_DRAINED`,
   * `JOB_STATE_CANCELLED`, or `JOB_STATE_FAILED`.
   */
  JOB_STATE_DRAINING = 7,
  /**
   * JOB_STATE_DRAINED - `JOB_STATE_DRAINED` indicates that the job has been drained.
   * A drained job terminated by stopping pulling from its input sources and
   * processing any data that remained in-flight when draining was requested.
   * This state is a terminal state, may only be set by the Cloud Dataflow
   * service, and only as a transition from `JOB_STATE_DRAINING`.
   */
  JOB_STATE_DRAINED = 8,
  /**
   * JOB_STATE_PENDING - `JOB_STATE_PENDING` indicates that the job has been created but is not yet
   * running.  Jobs that are pending may only transition to `JOB_STATE_RUNNING`,
   * or `JOB_STATE_FAILED`.
   */
  JOB_STATE_PENDING = 9,
  /**
   * JOB_STATE_CANCELLING - `JOB_STATE_CANCELLING` indicates that the job has been explicitly cancelled
   * and is in the process of stopping.  Jobs that are cancelling may only
   * transition to `JOB_STATE_CANCELLED` or `JOB_STATE_FAILED`.
   */
  JOB_STATE_CANCELLING = 10,
  /**
   * JOB_STATE_QUEUED - `JOB_STATE_QUEUED` indicates that the job has been created but is being
   * delayed until launch. Jobs that are queued may only transition to
   * `JOB_STATE_PENDING` or `JOB_STATE_CANCELLED`.
   */
  JOB_STATE_QUEUED = 11,
  /**
   * JOB_STATE_RESOURCE_CLEANING_UP - `JOB_STATE_RESOURCE_CLEANING_UP` indicates that the batch job's associated
   * resources are currently being cleaned up after a successful run.
   * Currently, this is an opt-in feature, please reach out to Cloud support
   * team if you are interested.
   */
  JOB_STATE_RESOURCE_CLEANING_UP = 12,
  UNRECOGNIZED = -1,
}

export function jobStateFromJSON(object: any): JobState {
  switch (object) {
    case 0:
    case "JOB_STATE_UNKNOWN":
      return JobState.JOB_STATE_UNKNOWN;
    case 1:
    case "JOB_STATE_STOPPED":
      return JobState.JOB_STATE_STOPPED;
    case 2:
    case "JOB_STATE_RUNNING":
      return JobState.JOB_STATE_RUNNING;
    case 3:
    case "JOB_STATE_DONE":
      return JobState.JOB_STATE_DONE;
    case 4:
    case "JOB_STATE_FAILED":
      return JobState.JOB_STATE_FAILED;
    case 5:
    case "JOB_STATE_CANCELLED":
      return JobState.JOB_STATE_CANCELLED;
    case 6:
    case "JOB_STATE_UPDATED":
      return JobState.JOB_STATE_UPDATED;
    case 7:
    case "JOB_STATE_DRAINING":
      return JobState.JOB_STATE_DRAINING;
    case 8:
    case "JOB_STATE_DRAINED":
      return JobState.JOB_STATE_DRAINED;
    case 9:
    case "JOB_STATE_PENDING":
      return JobState.JOB_STATE_PENDING;
    case 10:
    case "JOB_STATE_CANCELLING":
      return JobState.JOB_STATE_CANCELLING;
    case 11:
    case "JOB_STATE_QUEUED":
      return JobState.JOB_STATE_QUEUED;
    case 12:
    case "JOB_STATE_RESOURCE_CLEANING_UP":
      return JobState.JOB_STATE_RESOURCE_CLEANING_UP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobState.UNRECOGNIZED;
  }
}

export function jobStateToJSON(object: JobState): string {
  switch (object) {
    case JobState.JOB_STATE_UNKNOWN:
      return "JOB_STATE_UNKNOWN";
    case JobState.JOB_STATE_STOPPED:
      return "JOB_STATE_STOPPED";
    case JobState.JOB_STATE_RUNNING:
      return "JOB_STATE_RUNNING";
    case JobState.JOB_STATE_DONE:
      return "JOB_STATE_DONE";
    case JobState.JOB_STATE_FAILED:
      return "JOB_STATE_FAILED";
    case JobState.JOB_STATE_CANCELLED:
      return "JOB_STATE_CANCELLED";
    case JobState.JOB_STATE_UPDATED:
      return "JOB_STATE_UPDATED";
    case JobState.JOB_STATE_DRAINING:
      return "JOB_STATE_DRAINING";
    case JobState.JOB_STATE_DRAINED:
      return "JOB_STATE_DRAINED";
    case JobState.JOB_STATE_PENDING:
      return "JOB_STATE_PENDING";
    case JobState.JOB_STATE_CANCELLING:
      return "JOB_STATE_CANCELLING";
    case JobState.JOB_STATE_QUEUED:
      return "JOB_STATE_QUEUED";
    case JobState.JOB_STATE_RESOURCE_CLEANING_UP:
      return "JOB_STATE_RESOURCE_CLEANING_UP";
    case JobState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Describes the environment in which a Dataflow Job runs. */
export interface Environment {
  /**
   * The prefix of the resources the system should use for temporary
   * storage.  The system will append the suffix "/temp-{JOBNAME} to
   * this resource prefix, where {JOBNAME} is the value of the
   * job_name field.  The resulting bucket and object prefix is used
   * as the prefix of the resources used to store temporary data
   * needed during the job execution.  NOTE: This will override the
   * value in taskrunner_settings.
   * The supported resource type is:
   *
   * Google Cloud Storage:
   *
   *   storage.googleapis.com/{bucket}/{object}
   *   bucket.storage.googleapis.com/{object}
   */
  tempStoragePrefix: string;
  /**
   * The type of cluster manager API to use.  If unknown or
   * unspecified, the service will attempt to choose a reasonable
   * default.  This should be in the form of the API service name,
   * e.g. "compute.googleapis.com".
   */
  clusterManagerApiService: string;
  /**
   * The list of experiments to enable. This field should be used for SDK
   * related experiments and not for service related experiments. The proper
   * field for service related experiments is service_options.
   */
  experiments: string[];
  /**
   * The list of service options to enable. This field should be used for
   * service related experiments only. These experiments, when graduating to GA,
   * should be replaced by dedicated fields or become default (i.e. always on).
   */
  serviceOptions: string[];
  /**
   * If set, contains the Cloud KMS key identifier used to encrypt data
   * at rest, AKA a Customer Managed Encryption Key (CMEK).
   *
   * Format:
   *   projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
   */
  serviceKmsKeyName: string;
  /**
   * The worker pools. At least one "harness" worker pool must be
   * specified in order for the job to have workers.
   */
  workerPools: WorkerPool[];
  /** A description of the process that generated the request. */
  userAgent?:
    | { [key: string]: any }
    | undefined;
  /**
   * A structure describing which components and their versions of the service
   * are required in order to run the job.
   */
  version?:
    | { [key: string]: any }
    | undefined;
  /**
   * The dataset for the current project where various workflow
   * related tables are stored.
   *
   * The supported resource type is:
   *
   * Google BigQuery:
   *   bigquery.googleapis.com/{dataset}
   */
  dataset: string;
  /**
   * The Cloud Dataflow SDK pipeline options specified by the user. These
   * options are passed through the service and are used to recreate the
   * SDK pipeline options on the worker in a language agnostic and platform
   * independent way.
   */
  sdkPipelineOptions?:
    | { [key: string]: any }
    | undefined;
  /** Identity to run virtual machines as. Defaults to the default account. */
  serviceAccountEmail: string;
  /** Which Flexible Resource Scheduling mode to run in. */
  flexResourceSchedulingGoal: FlexResourceSchedulingGoal;
  /**
   * The Compute Engine region
   * (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
   * which worker processing should occur, e.g. "us-west1". Mutually exclusive
   * with worker_zone. If neither worker_region nor worker_zone is specified,
   * default to the control plane's region.
   */
  workerRegion: string;
  /**
   * The Compute Engine zone
   * (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
   * which worker processing should occur, e.g. "us-west1-a". Mutually exclusive
   * with worker_region. If neither worker_region nor worker_zone is specified,
   * a zone in the control plane's region is chosen based on available capacity.
   */
  workerZone: string;
  /** Output only. The shuffle mode used for the job. */
  shuffleMode: ShuffleMode;
  /** Any debugging options to be supplied to the job. */
  debugOptions?: DebugOptions | undefined;
}

/**
 * The packages that must be installed in order for a worker to run the
 * steps of the Cloud Dataflow job that will be assigned to its worker
 * pool.
 *
 * This is the mechanism by which the Cloud Dataflow SDK causes code to
 * be loaded onto the workers. For example, the Cloud Dataflow Java SDK
 * might use this to install jars containing the user's code and all of the
 * various dependencies (libraries, data files, etc.) required in order
 * for that code to run.
 */
export interface Package {
  /** The name of the package. */
  name: string;
  /**
   * The resource to read the package from. The supported resource type is:
   *
   * Google Cloud Storage:
   *
   *   storage.googleapis.com/{bucket}
   *   bucket.storage.googleapis.com/
   */
  location: string;
}

/** Settings for WorkerPool autoscaling. */
export interface AutoscalingSettings {
  /** The algorithm to use for autoscaling. */
  algorithm: AutoscalingAlgorithm;
  /** The maximum number of workers to cap scaling at. */
  maxNumWorkers: number;
}

/** Defines an SDK harness container for executing Dataflow pipelines. */
export interface SdkHarnessContainerImage {
  /** A docker container image that resides in Google Container Registry. */
  containerImage: string;
  /**
   * If true, recommends the Dataflow service to use only one core per SDK
   * container instance with this image. If false (or unset) recommends using
   * more than one core per SDK container instance with this image for
   * efficiency. Note that Dataflow service may choose to override this property
   * if needed.
   */
  useSingleCorePerContainer: boolean;
  /**
   * Environment ID for the Beam runner API proto Environment that corresponds
   * to the current SDK Harness.
   */
  environmentId: string;
  /**
   * The set of capabilities enumerated in the above Environment proto. See also
   * [beam_runner_api.proto](https://github.com/apache/beam/blob/master/model/pipeline/src/main/proto/org/apache/beam/model/pipeline/v1/beam_runner_api.proto)
   */
  capabilities: string[];
}

/**
 * Describes one particular pool of Cloud Dataflow workers to be
 * instantiated by the Cloud Dataflow service in order to perform the
 * computations required by a job.  Note that a workflow job may use
 * multiple pools, in order to match the various computational
 * requirements of the various stages of the job.
 */
export interface WorkerPool {
  /**
   * The kind of the worker pool; currently only `harness` and `shuffle`
   * are supported.
   */
  kind: string;
  /**
   * Number of Google Compute Engine workers in this pool needed to
   * execute the job.  If zero or unspecified, the service will
   * attempt to choose a reasonable default.
   */
  numWorkers: number;
  /** Packages to be installed on workers. */
  packages: Package[];
  /**
   * The default package set to install.  This allows the service to
   * select a default set of packages which are useful to worker
   * harnesses written in a particular language.
   */
  defaultPackageSet: DefaultPackageSet;
  /**
   * Machine type (e.g. "n1-standard-1").  If empty or unspecified, the
   * service will attempt to choose a reasonable default.
   */
  machineType: string;
  /**
   * Sets the policy for determining when to turndown worker pool.
   * Allowed values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and
   * `TEARDOWN_NEVER`.
   * `TEARDOWN_ALWAYS` means workers are always torn down regardless of whether
   * the job succeeds. `TEARDOWN_ON_SUCCESS` means workers are torn down
   * if the job succeeds. `TEARDOWN_NEVER` means the workers are never torn
   * down.
   *
   * If the workers are not torn down by the service, they will
   * continue to run and use Google Compute Engine VM resources in the
   * user's project until they are explicitly terminated by the user.
   * Because of this, Google recommends using the `TEARDOWN_ALWAYS`
   * policy except for small, manually supervised test jobs.
   *
   * If unknown or unspecified, the service will attempt to choose a reasonable
   * default.
   */
  teardownPolicy: TeardownPolicy;
  /**
   * Size of root disk for VMs, in GB.  If zero or unspecified, the service will
   * attempt to choose a reasonable default.
   */
  diskSizeGb: number;
  /**
   * Type of root disk for VMs.  If empty or unspecified, the service will
   * attempt to choose a reasonable default.
   */
  diskType: string;
  /** Fully qualified source image for disks. */
  diskSourceImage: string;
  /**
   * Zone to run the worker pools in.  If empty or unspecified, the service
   * will attempt to choose a reasonable default.
   */
  zone: string;
  /**
   * The action to take on host maintenance, as defined by the Google
   * Compute Engine API.
   */
  onHostMaintenance: string;
  /** Metadata to set on the Google Compute Engine VMs. */
  metadata: { [key: string]: string };
  /** Settings for autoscaling of this WorkerPool. */
  autoscalingSettings?:
    | AutoscalingSettings
    | undefined;
  /**
   * Network to which VMs will be assigned.  If empty or unspecified,
   * the service will use the network "default".
   */
  network: string;
  /**
   * Subnetwork to which VMs will be assigned, if desired.  Expected to be of
   * the form "regions/REGION/subnetworks/SUBNETWORK".
   */
  subnetwork: string;
  /**
   * Required. Docker container image that executes the Cloud Dataflow worker
   * harness, residing in Google Container Registry.
   *
   * Deprecated for the Fn API path. Use sdk_harness_container_images instead.
   */
  workerHarnessContainerImage: string;
  /**
   * The number of threads per worker harness. If empty or unspecified, the
   * service will choose a number of threads (according to the number of cores
   * on the selected machine type for batch, or 1 by convention for streaming).
   */
  numThreadsPerWorker: number;
  /** Configuration for VM IPs. */
  ipConfiguration: WorkerIPAddressConfiguration;
  /**
   * Set of SDK harness containers needed to execute this pipeline. This will
   * only be set in the Fn API path. For non-cross-language pipelines this
   * should have only one entry. Cross-language pipelines will have two or more
   * entries.
   */
  sdkHarnessContainerImages: SdkHarnessContainerImage[];
}

export interface WorkerPool_MetadataEntry {
  key: string;
  value: string;
}

/** Describes any options that have an effect on the debugging of pipelines. */
export interface DebugOptions {
  /**
   * When true, enables the logging of the literal hot key to the user's Cloud
   * Logging.
   */
  enableHotKeyLogging: boolean;
}

/**
 * Defines a job to be run by the Cloud Dataflow service. Do not enter
 * confidential information when you supply string values using the API.
 * Fields stripped from source Job proto:
 * - steps
 * - pipeline_description
 * - transform_name_mapping
 */
export interface Job {
  /**
   * The unique ID of this job.
   *
   * This field is set by the Cloud Dataflow service when the Job is
   * created, and is immutable for the life of the job.
   */
  id: string;
  /** The ID of the Cloud Platform project that the job belongs to. */
  projectId: string;
  /**
   * The user-specified Cloud Dataflow job name.
   *
   * Only one Job with a given name can exist in a project within one region at
   * any given time. Jobs in different regions can have the same name.
   * If a caller attempts to create a Job with the same
   * name as an already-existing Job, the attempt returns the
   * existing Job.
   *
   * The name must match the regular expression
   * `[a-z]([-a-z0-9]{0,1022}[a-z0-9])?`
   */
  name: string;
  /** The type of Cloud Dataflow job. */
  type: JobType;
  /** The environment for the job. */
  environment?:
    | Environment
    | undefined;
  /** The Cloud Storage location where the steps are stored. */
  stepsLocation: string;
  /**
   * The current state of the job.
   *
   * Jobs are created in the `JOB_STATE_STOPPED` state unless otherwise
   * specified.
   *
   * A job in the `JOB_STATE_RUNNING` state may asynchronously enter a
   * terminal state. After a job has reached a terminal state, no
   * further state updates may be made.
   *
   * This field may be mutated by the Cloud Dataflow service;
   * callers cannot mutate it.
   */
  currentState: JobState;
  /** The timestamp associated with the current state. */
  currentStateTime?:
    | Date
    | undefined;
  /**
   * The job's requested state.
   *
   * `UpdateJob` may be used to switch between the `JOB_STATE_STOPPED` and
   * `JOB_STATE_RUNNING` states, by setting requested_state.  `UpdateJob` may
   * also be used to directly set a job's requested state to
   * `JOB_STATE_CANCELLED` or `JOB_STATE_DONE`, irrevocably terminating the
   * job if it has not already reached a terminal state.
   */
  requestedState: JobState;
  /** Deprecated. */
  executionInfo?:
    | JobExecutionInfo
    | undefined;
  /**
   * The timestamp when the job was initially created. Immutable and set by the
   * Cloud Dataflow service.
   */
  createTime?:
    | Date
    | undefined;
  /**
   * If this job is an update of an existing job, this field is the job ID
   * of the job it replaced.
   *
   * When sending a `CreateJobRequest`, you can update a job by specifying it
   * here. The job named here is stopped, and its intermediate state is
   * transferred to this job.
   */
  replaceJobId: string;
  /**
   * The client's unique identifier of the job, re-used across retried attempts.
   * If this field is set, the service will ensure its uniqueness.
   * The request to create a job will fail if the service has knowledge of a
   * previously submitted job with the same client's ID and job name.
   * The caller may use this field to ensure idempotence of job
   * creation across retried attempts to create a job.
   * By default, the field is empty and, in that case, the service ignores it.
   */
  clientRequestId: string;
  /**
   * If another job is an update of this job (and thus, this job is in
   * `JOB_STATE_UPDATED`), this field contains the ID of that job.
   */
  replacedByJobId: string;
  /**
   * A set of files the system should be aware of that are used
   * for temporary storage. These temporary files will be
   * removed on job completion.
   * No duplicates are allowed.
   * No file patterns are supported.
   *
   * The supported files are:
   *
   * Google Cloud Storage:
   *
   *    storage.googleapis.com/{bucket}/{object}
   *    bucket.storage.googleapis.com/{object}
   */
  tempFiles: string[];
  /**
   * User-defined labels for this job.
   *
   * The labels map can contain no more than 64 entries.  Entries of the labels
   * map are UTF8 strings that comply with the following restrictions:
   *
   * * Keys must conform to regexp:  [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}
   * * Values must conform to regexp:  [\p{Ll}\p{Lo}\p{N}_-]{0,63}
   * * Both keys and values are additionally constrained to be <= 128 bytes in
   * size.
   */
  labels: { [key: string]: string };
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains this job.
   */
  location: string;
  /**
   * This field may be mutated by the Cloud Dataflow service;
   * callers cannot mutate it.
   */
  stageStates: ExecutionStageState[];
  /**
   * This field is populated by the Dataflow service to support filtering jobs
   * by the metadata values provided here. Populated for ListJobs and all GetJob
   * views SUMMARY and higher.
   */
  jobMetadata?:
    | JobMetadata
    | undefined;
  /**
   * The timestamp when the job was started (transitioned to JOB_STATE_PENDING).
   * Flexible resource scheduling jobs are started with some delay after job
   * creation, so start_time is unset before start and is updated when the
   * job is started by the Cloud Dataflow service. For other jobs, start_time
   * always equals to create_time and is immutable and set by the Cloud Dataflow
   * service.
   */
  startTime?:
    | Date
    | undefined;
  /**
   * If this is specified, the job's initial state is populated from the given
   * snapshot.
   */
  createdFromSnapshotId: string;
  /**
   * Reserved for future use. This field is set only in responses from the
   * server; it is ignored if it is set in any requests.
   */
  satisfiesPzs: boolean;
}

export interface Job_LabelsEntry {
  key: string;
  value: string;
}

/** Metadata for a Datastore connector used by the job. */
export interface DatastoreIODetails {
  /** Namespace used in the connection. */
  namespace: string;
  /** ProjectId accessed in the connection. */
  projectId: string;
}

/** Metadata for a Pub/Sub connector used by the job. */
export interface PubSubIODetails {
  /** Topic accessed in the connection. */
  topic: string;
  /** Subscription used in the connection. */
  subscription: string;
}

/** Metadata for a File connector used by the job. */
export interface FileIODetails {
  /** File Pattern used to access files by the connector. */
  filePattern: string;
}

/** Metadata for a Cloud Bigtable connector used by the job. */
export interface BigTableIODetails {
  /** ProjectId accessed in the connection. */
  projectId: string;
  /** InstanceId accessed in the connection. */
  instanceId: string;
  /** TableId accessed in the connection. */
  tableId: string;
}

/** Metadata for a BigQuery connector used by the job. */
export interface BigQueryIODetails {
  /** Table accessed in the connection. */
  table: string;
  /** Dataset accessed in the connection. */
  dataset: string;
  /** Project accessed in the connection. */
  projectId: string;
  /** Query used to access data in the connection. */
  query: string;
}

/** Metadata for a Spanner connector used by the job. */
export interface SpannerIODetails {
  /** ProjectId accessed in the connection. */
  projectId: string;
  /** InstanceId accessed in the connection. */
  instanceId: string;
  /** DatabaseId accessed in the connection. */
  databaseId: string;
}

/** The version of the SDK used to run the job. */
export interface SdkVersion {
  /** The version of the SDK used to run the job. */
  version: string;
  /** A readable string describing the version of the SDK. */
  versionDisplayName: string;
  /** The support status for this SDK version. */
  sdkSupportStatus: SdkVersion_SdkSupportStatus;
}

/** The support status of the SDK used to run the job. */
export enum SdkVersion_SdkSupportStatus {
  /** UNKNOWN - Cloud Dataflow is unaware of this version. */
  UNKNOWN = 0,
  /** SUPPORTED - This is a known version of an SDK, and is supported. */
  SUPPORTED = 1,
  /** STALE - A newer version of the SDK family exists, and an update is recommended. */
  STALE = 2,
  /**
   * DEPRECATED - This version of the SDK is deprecated and will eventually be
   * unsupported.
   */
  DEPRECATED = 3,
  /** UNSUPPORTED - Support for this SDK version has ended and it should no longer be used. */
  UNSUPPORTED = 4,
  UNRECOGNIZED = -1,
}

export function sdkVersion_SdkSupportStatusFromJSON(object: any): SdkVersion_SdkSupportStatus {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return SdkVersion_SdkSupportStatus.UNKNOWN;
    case 1:
    case "SUPPORTED":
      return SdkVersion_SdkSupportStatus.SUPPORTED;
    case 2:
    case "STALE":
      return SdkVersion_SdkSupportStatus.STALE;
    case 3:
    case "DEPRECATED":
      return SdkVersion_SdkSupportStatus.DEPRECATED;
    case 4:
    case "UNSUPPORTED":
      return SdkVersion_SdkSupportStatus.UNSUPPORTED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SdkVersion_SdkSupportStatus.UNRECOGNIZED;
  }
}

export function sdkVersion_SdkSupportStatusToJSON(object: SdkVersion_SdkSupportStatus): string {
  switch (object) {
    case SdkVersion_SdkSupportStatus.UNKNOWN:
      return "UNKNOWN";
    case SdkVersion_SdkSupportStatus.SUPPORTED:
      return "SUPPORTED";
    case SdkVersion_SdkSupportStatus.STALE:
      return "STALE";
    case SdkVersion_SdkSupportStatus.DEPRECATED:
      return "DEPRECATED";
    case SdkVersion_SdkSupportStatus.UNSUPPORTED:
      return "UNSUPPORTED";
    case SdkVersion_SdkSupportStatus.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Metadata available primarily for filtering jobs. Will be included in the
 * ListJob response and Job SUMMARY view.
 */
export interface JobMetadata {
  /** The SDK version used to run the job. */
  sdkVersion?:
    | SdkVersion
    | undefined;
  /** Identification of a Spanner source used in the Dataflow job. */
  spannerDetails: SpannerIODetails[];
  /** Identification of a BigQuery source used in the Dataflow job. */
  bigqueryDetails: BigQueryIODetails[];
  /** Identification of a Cloud Bigtable source used in the Dataflow job. */
  bigTableDetails: BigTableIODetails[];
  /** Identification of a Pub/Sub source used in the Dataflow job. */
  pubsubDetails: PubSubIODetails[];
  /** Identification of a File source used in the Dataflow job. */
  fileDetails: FileIODetails[];
  /** Identification of a Datastore source used in the Dataflow job. */
  datastoreDetails: DatastoreIODetails[];
}

/** A message describing the state of a particular execution stage. */
export interface ExecutionStageState {
  /** The name of the execution stage. */
  executionStageName: string;
  /** Executions stage states allow the same set of values as JobState. */
  executionStageState: JobState;
  /** The time at which the stage transitioned to this state. */
  currentStateTime?: Date | undefined;
}

/**
 * Additional information about how a Cloud Dataflow job will be executed that
 * isn't contained in the submitted job.
 */
export interface JobExecutionInfo {
  /** A mapping from each stage to the information about that stage. */
  stages: { [key: string]: JobExecutionStageInfo };
}

export interface JobExecutionInfo_StagesEntry {
  key: string;
  value?: JobExecutionStageInfo | undefined;
}

/**
 * Contains information about how a particular
 * [google.dataflow.v1beta3.Step][google.dataflow.v1beta3.Step] will be
 * executed.
 */
export interface JobExecutionStageInfo {
  /**
   * The steps associated with the execution stage.
   * Note that stages may have several steps, and that a given step
   * might be run by more than one stage.
   */
  stepName: string[];
}

/** The data within all Job events. */
export interface JobEventData {
  /** The Job event payload. */
  payload?: Job | undefined;
}

function createBaseEnvironment(): Environment {
  return {
    tempStoragePrefix: "",
    clusterManagerApiService: "",
    experiments: [],
    serviceOptions: [],
    serviceKmsKeyName: "",
    workerPools: [],
    userAgent: undefined,
    version: undefined,
    dataset: "",
    sdkPipelineOptions: undefined,
    serviceAccountEmail: "",
    flexResourceSchedulingGoal: 0,
    workerRegion: "",
    workerZone: "",
    shuffleMode: 0,
    debugOptions: undefined,
  };
}

export const Environment: MessageFns<Environment> = {
  encode(message: Environment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tempStoragePrefix !== "") {
      writer.uint32(10).string(message.tempStoragePrefix);
    }
    if (message.clusterManagerApiService !== "") {
      writer.uint32(18).string(message.clusterManagerApiService);
    }
    for (const v of message.experiments) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.serviceOptions) {
      writer.uint32(130).string(v!);
    }
    if (message.serviceKmsKeyName !== "") {
      writer.uint32(98).string(message.serviceKmsKeyName);
    }
    for (const v of message.workerPools) {
      WorkerPool.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.userAgent !== undefined) {
      Struct.encode(Struct.wrap(message.userAgent), writer.uint32(42).fork()).join();
    }
    if (message.version !== undefined) {
      Struct.encode(Struct.wrap(message.version), writer.uint32(50).fork()).join();
    }
    if (message.dataset !== "") {
      writer.uint32(58).string(message.dataset);
    }
    if (message.sdkPipelineOptions !== undefined) {
      Struct.encode(Struct.wrap(message.sdkPipelineOptions), writer.uint32(66).fork()).join();
    }
    if (message.serviceAccountEmail !== "") {
      writer.uint32(82).string(message.serviceAccountEmail);
    }
    if (message.flexResourceSchedulingGoal !== 0) {
      writer.uint32(88).int32(message.flexResourceSchedulingGoal);
    }
    if (message.workerRegion !== "") {
      writer.uint32(106).string(message.workerRegion);
    }
    if (message.workerZone !== "") {
      writer.uint32(114).string(message.workerZone);
    }
    if (message.shuffleMode !== 0) {
      writer.uint32(120).int32(message.shuffleMode);
    }
    if (message.debugOptions !== undefined) {
      DebugOptions.encode(message.debugOptions, writer.uint32(138).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Environment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEnvironment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.tempStoragePrefix = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.clusterManagerApiService = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.experiments.push(reader.string());
          continue;
        }
        case 16: {
          if (tag !== 130) {
            break;
          }

          message.serviceOptions.push(reader.string());
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.serviceKmsKeyName = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.workerPools.push(WorkerPool.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.userAgent = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.version = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.dataset = reader.string();
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.sdkPipelineOptions = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.serviceAccountEmail = reader.string();
          continue;
        }
        case 11: {
          if (tag !== 88) {
            break;
          }

          message.flexResourceSchedulingGoal = reader.int32() as any;
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.workerRegion = reader.string();
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.workerZone = reader.string();
          continue;
        }
        case 15: {
          if (tag !== 120) {
            break;
          }

          message.shuffleMode = reader.int32() as any;
          continue;
        }
        case 17: {
          if (tag !== 138) {
            break;
          }

          message.debugOptions = DebugOptions.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Environment {
    return {
      tempStoragePrefix: isSet(object.tempStoragePrefix) ? globalThis.String(object.tempStoragePrefix) : "",
      clusterManagerApiService: isSet(object.clusterManagerApiService)
        ? globalThis.String(object.clusterManagerApiService)
        : "",
      experiments: globalThis.Array.isArray(object?.experiments)
        ? object.experiments.map((e: any) => globalThis.String(e))
        : [],
      serviceOptions: globalThis.Array.isArray(object?.serviceOptions)
        ? object.serviceOptions.map((e: any) => globalThis.String(e))
        : [],
      serviceKmsKeyName: isSet(object.serviceKmsKeyName) ? globalThis.String(object.serviceKmsKeyName) : "",
      workerPools: globalThis.Array.isArray(object?.workerPools)
        ? object.workerPools.map((e: any) => WorkerPool.fromJSON(e))
        : [],
      userAgent: isObject(object.userAgent) ? object.userAgent : undefined,
      version: isObject(object.version) ? object.version : undefined,
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      sdkPipelineOptions: isObject(object.sdkPipelineOptions) ? object.sdkPipelineOptions : undefined,
      serviceAccountEmail: isSet(object.serviceAccountEmail) ? globalThis.String(object.serviceAccountEmail) : "",
      flexResourceSchedulingGoal: isSet(object.flexResourceSchedulingGoal)
        ? flexResourceSchedulingGoalFromJSON(object.flexResourceSchedulingGoal)
        : 0,
      workerRegion: isSet(object.workerRegion) ? globalThis.String(object.workerRegion) : "",
      workerZone: isSet(object.workerZone) ? globalThis.String(object.workerZone) : "",
      shuffleMode: isSet(object.shuffleMode) ? shuffleModeFromJSON(object.shuffleMode) : 0,
      debugOptions: isSet(object.debugOptions) ? DebugOptions.fromJSON(object.debugOptions) : undefined,
    };
  },

  toJSON(message: Environment): unknown {
    const obj: any = {};
    if (message.tempStoragePrefix !== "") {
      obj.tempStoragePrefix = message.tempStoragePrefix;
    }
    if (message.clusterManagerApiService !== "") {
      obj.clusterManagerApiService = message.clusterManagerApiService;
    }
    if (message.experiments?.length) {
      obj.experiments = message.experiments;
    }
    if (message.serviceOptions?.length) {
      obj.serviceOptions = message.serviceOptions;
    }
    if (message.serviceKmsKeyName !== "") {
      obj.serviceKmsKeyName = message.serviceKmsKeyName;
    }
    if (message.workerPools?.length) {
      obj.workerPools = message.workerPools.map((e) => WorkerPool.toJSON(e));
    }
    if (message.userAgent !== undefined) {
      obj.userAgent = message.userAgent;
    }
    if (message.version !== undefined) {
      obj.version = message.version;
    }
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.sdkPipelineOptions !== undefined) {
      obj.sdkPipelineOptions = message.sdkPipelineOptions;
    }
    if (message.serviceAccountEmail !== "") {
      obj.serviceAccountEmail = message.serviceAccountEmail;
    }
    if (message.flexResourceSchedulingGoal !== 0) {
      obj.flexResourceSchedulingGoal = flexResourceSchedulingGoalToJSON(message.flexResourceSchedulingGoal);
    }
    if (message.workerRegion !== "") {
      obj.workerRegion = message.workerRegion;
    }
    if (message.workerZone !== "") {
      obj.workerZone = message.workerZone;
    }
    if (message.shuffleMode !== 0) {
      obj.shuffleMode = shuffleModeToJSON(message.shuffleMode);
    }
    if (message.debugOptions !== undefined) {
      obj.debugOptions = DebugOptions.toJSON(message.debugOptions);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Environment>, I>>(base?: I): Environment {
    return Environment.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Environment>, I>>(object: I): Environment {
    const message = createBaseEnvironment();
    message.tempStoragePrefix = object.tempStoragePrefix ?? "";
    message.clusterManagerApiService = object.clusterManagerApiService ?? "";
    message.experiments = object.experiments?.map((e) => e) || [];
    message.serviceOptions = object.serviceOptions?.map((e) => e) || [];
    message.serviceKmsKeyName = object.serviceKmsKeyName ?? "";
    message.workerPools = object.workerPools?.map((e) => WorkerPool.fromPartial(e)) || [];
    message.userAgent = object.userAgent ?? undefined;
    message.version = object.version ?? undefined;
    message.dataset = object.dataset ?? "";
    message.sdkPipelineOptions = object.sdkPipelineOptions ?? undefined;
    message.serviceAccountEmail = object.serviceAccountEmail ?? "";
    message.flexResourceSchedulingGoal = object.flexResourceSchedulingGoal ?? 0;
    message.workerRegion = object.workerRegion ?? "";
    message.workerZone = object.workerZone ?? "";
    message.shuffleMode = object.shuffleMode ?? 0;
    message.debugOptions = (object.debugOptions !== undefined && object.debugOptions !== null)
      ? DebugOptions.fromPartial(object.debugOptions)
      : undefined;
    return message;
  },
};

function createBasePackage(): Package {
  return { name: "", location: "" };
}

export const Package: MessageFns<Package> = {
  encode(message: Package, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.location !== "") {
      writer.uint32(18).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Package {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePackage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.location = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Package {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: Package): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Package>, I>>(base?: I): Package {
    return Package.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Package>, I>>(object: I): Package {
    const message = createBasePackage();
    message.name = object.name ?? "";
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseAutoscalingSettings(): AutoscalingSettings {
  return { algorithm: 0, maxNumWorkers: 0 };
}

export const AutoscalingSettings: MessageFns<AutoscalingSettings> = {
  encode(message: AutoscalingSettings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.algorithm !== 0) {
      writer.uint32(8).int32(message.algorithm);
    }
    if (message.maxNumWorkers !== 0) {
      writer.uint32(16).int32(message.maxNumWorkers);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalingSettings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalingSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.algorithm = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.maxNumWorkers = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalingSettings {
    return {
      algorithm: isSet(object.algorithm) ? autoscalingAlgorithmFromJSON(object.algorithm) : 0,
      maxNumWorkers: isSet(object.maxNumWorkers) ? globalThis.Number(object.maxNumWorkers) : 0,
    };
  },

  toJSON(message: AutoscalingSettings): unknown {
    const obj: any = {};
    if (message.algorithm !== 0) {
      obj.algorithm = autoscalingAlgorithmToJSON(message.algorithm);
    }
    if (message.maxNumWorkers !== 0) {
      obj.maxNumWorkers = Math.round(message.maxNumWorkers);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AutoscalingSettings>, I>>(base?: I): AutoscalingSettings {
    return AutoscalingSettings.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AutoscalingSettings>, I>>(object: I): AutoscalingSettings {
    const message = createBaseAutoscalingSettings();
    message.algorithm = object.algorithm ?? 0;
    message.maxNumWorkers = object.maxNumWorkers ?? 0;
    return message;
  },
};

function createBaseSdkHarnessContainerImage(): SdkHarnessContainerImage {
  return { containerImage: "", useSingleCorePerContainer: false, environmentId: "", capabilities: [] };
}

export const SdkHarnessContainerImage: MessageFns<SdkHarnessContainerImage> = {
  encode(message: SdkHarnessContainerImage, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.containerImage !== "") {
      writer.uint32(10).string(message.containerImage);
    }
    if (message.useSingleCorePerContainer !== false) {
      writer.uint32(16).bool(message.useSingleCorePerContainer);
    }
    if (message.environmentId !== "") {
      writer.uint32(26).string(message.environmentId);
    }
    for (const v of message.capabilities) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SdkHarnessContainerImage {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSdkHarnessContainerImage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.containerImage = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.useSingleCorePerContainer = reader.bool();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.environmentId = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.capabilities.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SdkHarnessContainerImage {
    return {
      containerImage: isSet(object.containerImage) ? globalThis.String(object.containerImage) : "",
      useSingleCorePerContainer: isSet(object.useSingleCorePerContainer)
        ? globalThis.Boolean(object.useSingleCorePerContainer)
        : false,
      environmentId: isSet(object.environmentId) ? globalThis.String(object.environmentId) : "",
      capabilities: globalThis.Array.isArray(object?.capabilities)
        ? object.capabilities.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: SdkHarnessContainerImage): unknown {
    const obj: any = {};
    if (message.containerImage !== "") {
      obj.containerImage = message.containerImage;
    }
    if (message.useSingleCorePerContainer !== false) {
      obj.useSingleCorePerContainer = message.useSingleCorePerContainer;
    }
    if (message.environmentId !== "") {
      obj.environmentId = message.environmentId;
    }
    if (message.capabilities?.length) {
      obj.capabilities = message.capabilities;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SdkHarnessContainerImage>, I>>(base?: I): SdkHarnessContainerImage {
    return SdkHarnessContainerImage.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SdkHarnessContainerImage>, I>>(object: I): SdkHarnessContainerImage {
    const message = createBaseSdkHarnessContainerImage();
    message.containerImage = object.containerImage ?? "";
    message.useSingleCorePerContainer = object.useSingleCorePerContainer ?? false;
    message.environmentId = object.environmentId ?? "";
    message.capabilities = object.capabilities?.map((e) => e) || [];
    return message;
  },
};

function createBaseWorkerPool(): WorkerPool {
  return {
    kind: "",
    numWorkers: 0,
    packages: [],
    defaultPackageSet: 0,
    machineType: "",
    teardownPolicy: 0,
    diskSizeGb: 0,
    diskType: "",
    diskSourceImage: "",
    zone: "",
    onHostMaintenance: "",
    metadata: {},
    autoscalingSettings: undefined,
    network: "",
    subnetwork: "",
    workerHarnessContainerImage: "",
    numThreadsPerWorker: 0,
    ipConfiguration: 0,
    sdkHarnessContainerImages: [],
  };
}

export const WorkerPool: MessageFns<WorkerPool> = {
  encode(message: WorkerPool, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.numWorkers !== 0) {
      writer.uint32(16).int32(message.numWorkers);
    }
    for (const v of message.packages) {
      Package.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.defaultPackageSet !== 0) {
      writer.uint32(32).int32(message.defaultPackageSet);
    }
    if (message.machineType !== "") {
      writer.uint32(42).string(message.machineType);
    }
    if (message.teardownPolicy !== 0) {
      writer.uint32(48).int32(message.teardownPolicy);
    }
    if (message.diskSizeGb !== 0) {
      writer.uint32(56).int32(message.diskSizeGb);
    }
    if (message.diskType !== "") {
      writer.uint32(130).string(message.diskType);
    }
    if (message.diskSourceImage !== "") {
      writer.uint32(66).string(message.diskSourceImage);
    }
    if (message.zone !== "") {
      writer.uint32(74).string(message.zone);
    }
    if (message.onHostMaintenance !== "") {
      writer.uint32(90).string(message.onHostMaintenance);
    }
    Object.entries(message.metadata).forEach(([key, value]) => {
      WorkerPool_MetadataEntry.encode({ key: key as any, value }, writer.uint32(106).fork()).join();
    });
    if (message.autoscalingSettings !== undefined) {
      AutoscalingSettings.encode(message.autoscalingSettings, writer.uint32(114).fork()).join();
    }
    if (message.network !== "") {
      writer.uint32(138).string(message.network);
    }
    if (message.subnetwork !== "") {
      writer.uint32(154).string(message.subnetwork);
    }
    if (message.workerHarnessContainerImage !== "") {
      writer.uint32(146).string(message.workerHarnessContainerImage);
    }
    if (message.numThreadsPerWorker !== 0) {
      writer.uint32(160).int32(message.numThreadsPerWorker);
    }
    if (message.ipConfiguration !== 0) {
      writer.uint32(168).int32(message.ipConfiguration);
    }
    for (const v of message.sdkHarnessContainerImages) {
      SdkHarnessContainerImage.encode(v!, writer.uint32(178).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WorkerPool {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWorkerPool();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.numWorkers = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.packages.push(Package.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.defaultPackageSet = reader.int32() as any;
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.machineType = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 48) {
            break;
          }

          message.teardownPolicy = reader.int32() as any;
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.diskSizeGb = reader.int32();
          continue;
        }
        case 16: {
          if (tag !== 130) {
            break;
          }

          message.diskType = reader.string();
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.diskSourceImage = reader.string();
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.zone = reader.string();
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.onHostMaintenance = reader.string();
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          const entry13 = WorkerPool_MetadataEntry.decode(reader, reader.uint32());
          if (entry13.value !== undefined) {
            message.metadata[entry13.key] = entry13.value;
          }
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.autoscalingSettings = AutoscalingSettings.decode(reader, reader.uint32());
          continue;
        }
        case 17: {
          if (tag !== 138) {
            break;
          }

          message.network = reader.string();
          continue;
        }
        case 19: {
          if (tag !== 154) {
            break;
          }

          message.subnetwork = reader.string();
          continue;
        }
        case 18: {
          if (tag !== 146) {
            break;
          }

          message.workerHarnessContainerImage = reader.string();
          continue;
        }
        case 20: {
          if (tag !== 160) {
            break;
          }

          message.numThreadsPerWorker = reader.int32();
          continue;
        }
        case 21: {
          if (tag !== 168) {
            break;
          }

          message.ipConfiguration = reader.int32() as any;
          continue;
        }
        case 22: {
          if (tag !== 178) {
            break;
          }

          message.sdkHarnessContainerImages.push(SdkHarnessContainerImage.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WorkerPool {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      numWorkers: isSet(object.numWorkers) ? globalThis.Number(object.numWorkers) : 0,
      packages: globalThis.Array.isArray(object?.packages) ? object.packages.map((e: any) => Package.fromJSON(e)) : [],
      defaultPackageSet: isSet(object.defaultPackageSet) ? defaultPackageSetFromJSON(object.defaultPackageSet) : 0,
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      teardownPolicy: isSet(object.teardownPolicy) ? teardownPolicyFromJSON(object.teardownPolicy) : 0,
      diskSizeGb: isSet(object.diskSizeGb) ? globalThis.Number(object.diskSizeGb) : 0,
      diskType: isSet(object.diskType) ? globalThis.String(object.diskType) : "",
      diskSourceImage: isSet(object.diskSourceImage) ? globalThis.String(object.diskSourceImage) : "",
      zone: isSet(object.zone) ? globalThis.String(object.zone) : "",
      onHostMaintenance: isSet(object.onHostMaintenance) ? globalThis.String(object.onHostMaintenance) : "",
      metadata: isObject(object.metadata)
        ? Object.entries(object.metadata).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      autoscalingSettings: isSet(object.autoscalingSettings)
        ? AutoscalingSettings.fromJSON(object.autoscalingSettings)
        : undefined,
      network: isSet(object.network) ? globalThis.String(object.network) : "",
      subnetwork: isSet(object.subnetwork) ? globalThis.String(object.subnetwork) : "",
      workerHarnessContainerImage: isSet(object.workerHarnessContainerImage)
        ? globalThis.String(object.workerHarnessContainerImage)
        : "",
      numThreadsPerWorker: isSet(object.numThreadsPerWorker) ? globalThis.Number(object.numThreadsPerWorker) : 0,
      ipConfiguration: isSet(object.ipConfiguration) ? workerIPAddressConfigurationFromJSON(object.ipConfiguration) : 0,
      sdkHarnessContainerImages: globalThis.Array.isArray(object?.sdkHarnessContainerImages)
        ? object.sdkHarnessContainerImages.map((e: any) => SdkHarnessContainerImage.fromJSON(e))
        : [],
    };
  },

  toJSON(message: WorkerPool): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.numWorkers !== 0) {
      obj.numWorkers = Math.round(message.numWorkers);
    }
    if (message.packages?.length) {
      obj.packages = message.packages.map((e) => Package.toJSON(e));
    }
    if (message.defaultPackageSet !== 0) {
      obj.defaultPackageSet = defaultPackageSetToJSON(message.defaultPackageSet);
    }
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.teardownPolicy !== 0) {
      obj.teardownPolicy = teardownPolicyToJSON(message.teardownPolicy);
    }
    if (message.diskSizeGb !== 0) {
      obj.diskSizeGb = Math.round(message.diskSizeGb);
    }
    if (message.diskType !== "") {
      obj.diskType = message.diskType;
    }
    if (message.diskSourceImage !== "") {
      obj.diskSourceImage = message.diskSourceImage;
    }
    if (message.zone !== "") {
      obj.zone = message.zone;
    }
    if (message.onHostMaintenance !== "") {
      obj.onHostMaintenance = message.onHostMaintenance;
    }
    if (message.metadata) {
      const entries = Object.entries(message.metadata);
      if (entries.length > 0) {
        obj.metadata = {};
        entries.forEach(([k, v]) => {
          obj.metadata[k] = v;
        });
      }
    }
    if (message.autoscalingSettings !== undefined) {
      obj.autoscalingSettings = AutoscalingSettings.toJSON(message.autoscalingSettings);
    }
    if (message.network !== "") {
      obj.network = message.network;
    }
    if (message.subnetwork !== "") {
      obj.subnetwork = message.subnetwork;
    }
    if (message.workerHarnessContainerImage !== "") {
      obj.workerHarnessContainerImage = message.workerHarnessContainerImage;
    }
    if (message.numThreadsPerWorker !== 0) {
      obj.numThreadsPerWorker = Math.round(message.numThreadsPerWorker);
    }
    if (message.ipConfiguration !== 0) {
      obj.ipConfiguration = workerIPAddressConfigurationToJSON(message.ipConfiguration);
    }
    if (message.sdkHarnessContainerImages?.length) {
      obj.sdkHarnessContainerImages = message.sdkHarnessContainerImages.map((e) => SdkHarnessContainerImage.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<WorkerPool>, I>>(base?: I): WorkerPool {
    return WorkerPool.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<WorkerPool>, I>>(object: I): WorkerPool {
    const message = createBaseWorkerPool();
    message.kind = object.kind ?? "";
    message.numWorkers = object.numWorkers ?? 0;
    message.packages = object.packages?.map((e) => Package.fromPartial(e)) || [];
    message.defaultPackageSet = object.defaultPackageSet ?? 0;
    message.machineType = object.machineType ?? "";
    message.teardownPolicy = object.teardownPolicy ?? 0;
    message.diskSizeGb = object.diskSizeGb ?? 0;
    message.diskType = object.diskType ?? "";
    message.diskSourceImage = object.diskSourceImage ?? "";
    message.zone = object.zone ?? "";
    message.onHostMaintenance = object.onHostMaintenance ?? "";
    message.metadata = Object.entries(object.metadata ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.autoscalingSettings = (object.autoscalingSettings !== undefined && object.autoscalingSettings !== null)
      ? AutoscalingSettings.fromPartial(object.autoscalingSettings)
      : undefined;
    message.network = object.network ?? "";
    message.subnetwork = object.subnetwork ?? "";
    message.workerHarnessContainerImage = object.workerHarnessContainerImage ?? "";
    message.numThreadsPerWorker = object.numThreadsPerWorker ?? 0;
    message.ipConfiguration = object.ipConfiguration ?? 0;
    message.sdkHarnessContainerImages =
      object.sdkHarnessContainerImages?.map((e) => SdkHarnessContainerImage.fromPartial(e)) || [];
    return message;
  },
};

function createBaseWorkerPool_MetadataEntry(): WorkerPool_MetadataEntry {
  return { key: "", value: "" };
}

export const WorkerPool_MetadataEntry: MessageFns<WorkerPool_MetadataEntry> = {
  encode(message: WorkerPool_MetadataEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WorkerPool_MetadataEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWorkerPool_MetadataEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WorkerPool_MetadataEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: WorkerPool_MetadataEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<WorkerPool_MetadataEntry>, I>>(base?: I): WorkerPool_MetadataEntry {
    return WorkerPool_MetadataEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<WorkerPool_MetadataEntry>, I>>(object: I): WorkerPool_MetadataEntry {
    const message = createBaseWorkerPool_MetadataEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDebugOptions(): DebugOptions {
  return { enableHotKeyLogging: false };
}

export const DebugOptions: MessageFns<DebugOptions> = {
  encode(message: DebugOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableHotKeyLogging !== false) {
      writer.uint32(8).bool(message.enableHotKeyLogging);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DebugOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDebugOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.enableHotKeyLogging = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DebugOptions {
    return {
      enableHotKeyLogging: isSet(object.enableHotKeyLogging) ? globalThis.Boolean(object.enableHotKeyLogging) : false,
    };
  },

  toJSON(message: DebugOptions): unknown {
    const obj: any = {};
    if (message.enableHotKeyLogging !== false) {
      obj.enableHotKeyLogging = message.enableHotKeyLogging;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<DebugOptions>, I>>(base?: I): DebugOptions {
    return DebugOptions.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<DebugOptions>, I>>(object: I): DebugOptions {
    const message = createBaseDebugOptions();
    message.enableHotKeyLogging = object.enableHotKeyLogging ?? false;
    return message;
  },
};

function createBaseJob(): Job {
  return {
    id: "",
    projectId: "",
    name: "",
    type: 0,
    environment: undefined,
    stepsLocation: "",
    currentState: 0,
    currentStateTime: undefined,
    requestedState: 0,
    executionInfo: undefined,
    createTime: undefined,
    replaceJobId: "",
    clientRequestId: "",
    replacedByJobId: "",
    tempFiles: [],
    labels: {},
    location: "",
    stageStates: [],
    jobMetadata: undefined,
    startTime: undefined,
    createdFromSnapshotId: "",
    satisfiesPzs: false,
  };
}

export const Job: MessageFns<Job> = {
  encode(message: Job, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.type !== 0) {
      writer.uint32(32).int32(message.type);
    }
    if (message.environment !== undefined) {
      Environment.encode(message.environment, writer.uint32(42).fork()).join();
    }
    if (message.stepsLocation !== "") {
      writer.uint32(194).string(message.stepsLocation);
    }
    if (message.currentState !== 0) {
      writer.uint32(56).int32(message.currentState);
    }
    if (message.currentStateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.currentStateTime), writer.uint32(66).fork()).join();
    }
    if (message.requestedState !== 0) {
      writer.uint32(72).int32(message.requestedState);
    }
    if (message.executionInfo !== undefined) {
      JobExecutionInfo.encode(message.executionInfo, writer.uint32(82).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(90).fork()).join();
    }
    if (message.replaceJobId !== "") {
      writer.uint32(98).string(message.replaceJobId);
    }
    if (message.clientRequestId !== "") {
      writer.uint32(114).string(message.clientRequestId);
    }
    if (message.replacedByJobId !== "") {
      writer.uint32(122).string(message.replacedByJobId);
    }
    for (const v of message.tempFiles) {
      writer.uint32(130).string(v!);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Job_LabelsEntry.encode({ key: key as any, value }, writer.uint32(138).fork()).join();
    });
    if (message.location !== "") {
      writer.uint32(146).string(message.location);
    }
    for (const v of message.stageStates) {
      ExecutionStageState.encode(v!, writer.uint32(162).fork()).join();
    }
    if (message.jobMetadata !== undefined) {
      JobMetadata.encode(message.jobMetadata, writer.uint32(170).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(178).fork()).join();
    }
    if (message.createdFromSnapshotId !== "") {
      writer.uint32(186).string(message.createdFromSnapshotId);
    }
    if (message.satisfiesPzs !== false) {
      writer.uint32(200).bool(message.satisfiesPzs);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.environment = Environment.decode(reader, reader.uint32());
          continue;
        }
        case 24: {
          if (tag !== 194) {
            break;
          }

          message.stepsLocation = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.currentState = reader.int32() as any;
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.currentStateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 9: {
          if (tag !== 72) {
            break;
          }

          message.requestedState = reader.int32() as any;
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.executionInfo = JobExecutionInfo.decode(reader, reader.uint32());
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.replaceJobId = reader.string();
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.clientRequestId = reader.string();
          continue;
        }
        case 15: {
          if (tag !== 122) {
            break;
          }

          message.replacedByJobId = reader.string();
          continue;
        }
        case 16: {
          if (tag !== 130) {
            break;
          }

          message.tempFiles.push(reader.string());
          continue;
        }
        case 17: {
          if (tag !== 138) {
            break;
          }

          const entry17 = Job_LabelsEntry.decode(reader, reader.uint32());
          if (entry17.value !== undefined) {
            message.labels[entry17.key] = entry17.value;
          }
          continue;
        }
        case 18: {
          if (tag !== 146) {
            break;
          }

          message.location = reader.string();
          continue;
        }
        case 20: {
          if (tag !== 162) {
            break;
          }

          message.stageStates.push(ExecutionStageState.decode(reader, reader.uint32()));
          continue;
        }
        case 21: {
          if (tag !== 170) {
            break;
          }

          message.jobMetadata = JobMetadata.decode(reader, reader.uint32());
          continue;
        }
        case 22: {
          if (tag !== 178) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 23: {
          if (tag !== 186) {
            break;
          }

          message.createdFromSnapshotId = reader.string();
          continue;
        }
        case 25: {
          if (tag !== 200) {
            break;
          }

          message.satisfiesPzs = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? jobTypeFromJSON(object.type) : 0,
      environment: isSet(object.environment) ? Environment.fromJSON(object.environment) : undefined,
      stepsLocation: isSet(object.stepsLocation) ? globalThis.String(object.stepsLocation) : "",
      currentState: isSet(object.currentState) ? jobStateFromJSON(object.currentState) : 0,
      currentStateTime: isSet(object.currentStateTime) ? fromJsonTimestamp(object.currentStateTime) : undefined,
      requestedState: isSet(object.requestedState) ? jobStateFromJSON(object.requestedState) : 0,
      executionInfo: isSet(object.executionInfo) ? JobExecutionInfo.fromJSON(object.executionInfo) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      replaceJobId: isSet(object.replaceJobId) ? globalThis.String(object.replaceJobId) : "",
      clientRequestId: isSet(object.clientRequestId) ? globalThis.String(object.clientRequestId) : "",
      replacedByJobId: isSet(object.replacedByJobId) ? globalThis.String(object.replacedByJobId) : "",
      tempFiles: globalThis.Array.isArray(object?.tempFiles)
        ? object.tempFiles.map((e: any) => globalThis.String(e))
        : [],
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      stageStates: globalThis.Array.isArray(object?.stageStates)
        ? object.stageStates.map((e: any) => ExecutionStageState.fromJSON(e))
        : [],
      jobMetadata: isSet(object.jobMetadata) ? JobMetadata.fromJSON(object.jobMetadata) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      createdFromSnapshotId: isSet(object.createdFromSnapshotId) ? globalThis.String(object.createdFromSnapshotId) : "",
      satisfiesPzs: isSet(object.satisfiesPzs) ? globalThis.Boolean(object.satisfiesPzs) : false,
    };
  },

  toJSON(message: Job): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== 0) {
      obj.type = jobTypeToJSON(message.type);
    }
    if (message.environment !== undefined) {
      obj.environment = Environment.toJSON(message.environment);
    }
    if (message.stepsLocation !== "") {
      obj.stepsLocation = message.stepsLocation;
    }
    if (message.currentState !== 0) {
      obj.currentState = jobStateToJSON(message.currentState);
    }
    if (message.currentStateTime !== undefined) {
      obj.currentStateTime = message.currentStateTime.toISOString();
    }
    if (message.requestedState !== 0) {
      obj.requestedState = jobStateToJSON(message.requestedState);
    }
    if (message.executionInfo !== undefined) {
      obj.executionInfo = JobExecutionInfo.toJSON(message.executionInfo);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.replaceJobId !== "") {
      obj.replaceJobId = message.replaceJobId;
    }
    if (message.clientRequestId !== "") {
      obj.clientRequestId = message.clientRequestId;
    }
    if (message.replacedByJobId !== "") {
      obj.replacedByJobId = message.replacedByJobId;
    }
    if (message.tempFiles?.length) {
      obj.tempFiles = message.tempFiles;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.stageStates?.length) {
      obj.stageStates = message.stageStates.map((e) => ExecutionStageState.toJSON(e));
    }
    if (message.jobMetadata !== undefined) {
      obj.jobMetadata = JobMetadata.toJSON(message.jobMetadata);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.createdFromSnapshotId !== "") {
      obj.createdFromSnapshotId = message.createdFromSnapshotId;
    }
    if (message.satisfiesPzs !== false) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Job>, I>>(base?: I): Job {
    return Job.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Job>, I>>(object: I): Job {
    const message = createBaseJob();
    message.id = object.id ?? "";
    message.projectId = object.projectId ?? "";
    message.name = object.name ?? "";
    message.type = object.type ?? 0;
    message.environment = (object.environment !== undefined && object.environment !== null)
      ? Environment.fromPartial(object.environment)
      : undefined;
    message.stepsLocation = object.stepsLocation ?? "";
    message.currentState = object.currentState ?? 0;
    message.currentStateTime = object.currentStateTime ?? undefined;
    message.requestedState = object.requestedState ?? 0;
    message.executionInfo = (object.executionInfo !== undefined && object.executionInfo !== null)
      ? JobExecutionInfo.fromPartial(object.executionInfo)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.replaceJobId = object.replaceJobId ?? "";
    message.clientRequestId = object.clientRequestId ?? "";
    message.replacedByJobId = object.replacedByJobId ?? "";
    message.tempFiles = object.tempFiles?.map((e) => e) || [];
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.location = object.location ?? "";
    message.stageStates = object.stageStates?.map((e) => ExecutionStageState.fromPartial(e)) || [];
    message.jobMetadata = (object.jobMetadata !== undefined && object.jobMetadata !== null)
      ? JobMetadata.fromPartial(object.jobMetadata)
      : undefined;
    message.startTime = object.startTime ?? undefined;
    message.createdFromSnapshotId = object.createdFromSnapshotId ?? "";
    message.satisfiesPzs = object.satisfiesPzs ?? false;
    return message;
  },
};

function createBaseJob_LabelsEntry(): Job_LabelsEntry {
  return { key: "", value: "" };
}

export const Job_LabelsEntry: MessageFns<Job_LabelsEntry> = {
  encode(message: Job_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Job_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Job_LabelsEntry>, I>>(base?: I): Job_LabelsEntry {
    return Job_LabelsEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Job_LabelsEntry>, I>>(object: I): Job_LabelsEntry {
    const message = createBaseJob_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDatastoreIODetails(): DatastoreIODetails {
  return { namespace: "", projectId: "" };
}

export const DatastoreIODetails: MessageFns<DatastoreIODetails> = {
  encode(message: DatastoreIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.namespace !== "") {
      writer.uint32(10).string(message.namespace);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatastoreIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatastoreIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.namespace = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatastoreIODetails {
    return {
      namespace: isSet(object.namespace) ? globalThis.String(object.namespace) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
    };
  },

  toJSON(message: DatastoreIODetails): unknown {
    const obj: any = {};
    if (message.namespace !== "") {
      obj.namespace = message.namespace;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<DatastoreIODetails>, I>>(base?: I): DatastoreIODetails {
    return DatastoreIODetails.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<DatastoreIODetails>, I>>(object: I): DatastoreIODetails {
    const message = createBaseDatastoreIODetails();
    message.namespace = object.namespace ?? "";
    message.projectId = object.projectId ?? "";
    return message;
  },
};

function createBasePubSubIODetails(): PubSubIODetails {
  return { topic: "", subscription: "" };
}

export const PubSubIODetails: MessageFns<PubSubIODetails> = {
  encode(message: PubSubIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.topic !== "") {
      writer.uint32(10).string(message.topic);
    }
    if (message.subscription !== "") {
      writer.uint32(18).string(message.subscription);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PubSubIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePubSubIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.topic = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.subscription = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PubSubIODetails {
    return {
      topic: isSet(object.topic) ? globalThis.String(object.topic) : "",
      subscription: isSet(object.subscription) ? globalThis.String(object.subscription) : "",
    };
  },

  toJSON(message: PubSubIODetails): unknown {
    const obj: any = {};
    if (message.topic !== "") {
      obj.topic = message.topic;
    }
    if (message.subscription !== "") {
      obj.subscription = message.subscription;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PubSubIODetails>, I>>(base?: I): PubSubIODetails {
    return PubSubIODetails.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PubSubIODetails>, I>>(object: I): PubSubIODetails {
    const message = createBasePubSubIODetails();
    message.topic = object.topic ?? "";
    message.subscription = object.subscription ?? "";
    return message;
  },
};

function createBaseFileIODetails(): FileIODetails {
  return { filePattern: "" };
}

export const FileIODetails: MessageFns<FileIODetails> = {
  encode(message: FileIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.filePattern !== "") {
      writer.uint32(10).string(message.filePattern);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FileIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFileIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.filePattern = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FileIODetails {
    return { filePattern: isSet(object.filePattern) ? globalThis.String(object.filePattern) : "" };
  },

  toJSON(message: FileIODetails): unknown {
    const obj: any = {};
    if (message.filePattern !== "") {
      obj.filePattern = message.filePattern;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<FileIODetails>, I>>(base?: I): FileIODetails {
    return FileIODetails.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<FileIODetails>, I>>(object: I): FileIODetails {
    const message = createBaseFileIODetails();
    message.filePattern = object.filePattern ?? "";
    return message;
  },
};

function createBaseBigTableIODetails(): BigTableIODetails {
  return { projectId: "", instanceId: "", tableId: "" };
}

export const BigTableIODetails: MessageFns<BigTableIODetails> = {
  encode(message: BigTableIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.tableId !== "") {
      writer.uint32(26).string(message.tableId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigTableIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigTableIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.tableId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigTableIODetails {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
    };
  },

  toJSON(message: BigTableIODetails): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BigTableIODetails>, I>>(base?: I): BigTableIODetails {
    return BigTableIODetails.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BigTableIODetails>, I>>(object: I): BigTableIODetails {
    const message = createBaseBigTableIODetails();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.tableId = object.tableId ?? "";
    return message;
  },
};

function createBaseBigQueryIODetails(): BigQueryIODetails {
  return { table: "", dataset: "", projectId: "", query: "" };
}

export const BigQueryIODetails: MessageFns<BigQueryIODetails> = {
  encode(message: BigQueryIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.table !== "") {
      writer.uint32(10).string(message.table);
    }
    if (message.dataset !== "") {
      writer.uint32(18).string(message.dataset);
    }
    if (message.projectId !== "") {
      writer.uint32(26).string(message.projectId);
    }
    if (message.query !== "") {
      writer.uint32(34).string(message.query);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.table = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.dataset = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.query = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryIODetails {
    return {
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      query: isSet(object.query) ? globalThis.String(object.query) : "",
    };
  },

  toJSON(message: BigQueryIODetails): unknown {
    const obj: any = {};
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.query !== "") {
      obj.query = message.query;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<BigQueryIODetails>, I>>(base?: I): BigQueryIODetails {
    return BigQueryIODetails.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<BigQueryIODetails>, I>>(object: I): BigQueryIODetails {
    const message = createBaseBigQueryIODetails();
    message.table = object.table ?? "";
    message.dataset = object.dataset ?? "";
    message.projectId = object.projectId ?? "";
    message.query = object.query ?? "";
    return message;
  },
};

function createBaseSpannerIODetails(): SpannerIODetails {
  return { projectId: "", instanceId: "", databaseId: "" };
}

export const SpannerIODetails: MessageFns<SpannerIODetails> = {
  encode(message: SpannerIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.databaseId !== "") {
      writer.uint32(26).string(message.databaseId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpannerIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpannerIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.databaseId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpannerIODetails {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      databaseId: isSet(object.databaseId) ? globalThis.String(object.databaseId) : "",
    };
  },

  toJSON(message: SpannerIODetails): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.databaseId !== "") {
      obj.databaseId = message.databaseId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpannerIODetails>, I>>(base?: I): SpannerIODetails {
    return SpannerIODetails.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpannerIODetails>, I>>(object: I): SpannerIODetails {
    const message = createBaseSpannerIODetails();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.databaseId = object.databaseId ?? "";
    return message;
  },
};

function createBaseSdkVersion(): SdkVersion {
  return { version: "", versionDisplayName: "", sdkSupportStatus: 0 };
}

export const SdkVersion: MessageFns<SdkVersion> = {
  encode(message: SdkVersion, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.version !== "") {
      writer.uint32(10).string(message.version);
    }
    if (message.versionDisplayName !== "") {
      writer.uint32(18).string(message.versionDisplayName);
    }
    if (message.sdkSupportStatus !== 0) {
      writer.uint32(24).int32(message.sdkSupportStatus);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SdkVersion {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSdkVersion();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.version = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.versionDisplayName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.sdkSupportStatus = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SdkVersion {
    return {
      version: isSet(object.version) ? globalThis.String(object.version) : "",
      versionDisplayName: isSet(object.versionDisplayName) ? globalThis.String(object.versionDisplayName) : "",
      sdkSupportStatus: isSet(object.sdkSupportStatus)
        ? sdkVersion_SdkSupportStatusFromJSON(object.sdkSupportStatus)
        : 0,
    };
  },

  toJSON(message: SdkVersion): unknown {
    const obj: any = {};
    if (message.version !== "") {
      obj.version = message.version;
    }
    if (message.versionDisplayName !== "") {
      obj.versionDisplayName = message.versionDisplayName;
    }
    if (message.sdkSupportStatus !== 0) {
      obj.sdkSupportStatus = sdkVersion_SdkSupportStatusToJSON(message.sdkSupportStatus);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SdkVersion>, I>>(base?: I): SdkVersion {
    return SdkVersion.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SdkVersion>, I>>(object: I): SdkVersion {
    const message = createBaseSdkVersion();
    message.version = object.version ?? "";
    message.versionDisplayName = object.versionDisplayName ?? "";
    message.sdkSupportStatus = object.sdkSupportStatus ?? 0;
    return message;
  },
};

function createBaseJobMetadata(): JobMetadata {
  return {
    sdkVersion: undefined,
    spannerDetails: [],
    bigqueryDetails: [],
    bigTableDetails: [],
    pubsubDetails: [],
    fileDetails: [],
    datastoreDetails: [],
  };
}

export const JobMetadata: MessageFns<JobMetadata> = {
  encode(message: JobMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sdkVersion !== undefined) {
      SdkVersion.encode(message.sdkVersion, writer.uint32(10).fork()).join();
    }
    for (const v of message.spannerDetails) {
      SpannerIODetails.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.bigqueryDetails) {
      BigQueryIODetails.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.bigTableDetails) {
      BigTableIODetails.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.pubsubDetails) {
      PubSubIODetails.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.fileDetails) {
      FileIODetails.encode(v!, writer.uint32(50).fork()).join();
    }
    for (const v of message.datastoreDetails) {
      DatastoreIODetails.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.sdkVersion = SdkVersion.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.spannerDetails.push(SpannerIODetails.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.bigqueryDetails.push(BigQueryIODetails.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.bigTableDetails.push(BigTableIODetails.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.pubsubDetails.push(PubSubIODetails.decode(reader, reader.uint32()));
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.fileDetails.push(FileIODetails.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.datastoreDetails.push(DatastoreIODetails.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobMetadata {
    return {
      sdkVersion: isSet(object.sdkVersion) ? SdkVersion.fromJSON(object.sdkVersion) : undefined,
      spannerDetails: globalThis.Array.isArray(object?.spannerDetails)
        ? object.spannerDetails.map((e: any) => SpannerIODetails.fromJSON(e))
        : [],
      bigqueryDetails: globalThis.Array.isArray(object?.bigqueryDetails)
        ? object.bigqueryDetails.map((e: any) => BigQueryIODetails.fromJSON(e))
        : [],
      bigTableDetails: globalThis.Array.isArray(object?.bigTableDetails)
        ? object.bigTableDetails.map((e: any) => BigTableIODetails.fromJSON(e))
        : [],
      pubsubDetails: globalThis.Array.isArray(object?.pubsubDetails)
        ? object.pubsubDetails.map((e: any) => PubSubIODetails.fromJSON(e))
        : [],
      fileDetails: globalThis.Array.isArray(object?.fileDetails)
        ? object.fileDetails.map((e: any) => FileIODetails.fromJSON(e))
        : [],
      datastoreDetails: globalThis.Array.isArray(object?.datastoreDetails)
        ? object.datastoreDetails.map((e: any) => DatastoreIODetails.fromJSON(e))
        : [],
    };
  },

  toJSON(message: JobMetadata): unknown {
    const obj: any = {};
    if (message.sdkVersion !== undefined) {
      obj.sdkVersion = SdkVersion.toJSON(message.sdkVersion);
    }
    if (message.spannerDetails?.length) {
      obj.spannerDetails = message.spannerDetails.map((e) => SpannerIODetails.toJSON(e));
    }
    if (message.bigqueryDetails?.length) {
      obj.bigqueryDetails = message.bigqueryDetails.map((e) => BigQueryIODetails.toJSON(e));
    }
    if (message.bigTableDetails?.length) {
      obj.bigTableDetails = message.bigTableDetails.map((e) => BigTableIODetails.toJSON(e));
    }
    if (message.pubsubDetails?.length) {
      obj.pubsubDetails = message.pubsubDetails.map((e) => PubSubIODetails.toJSON(e));
    }
    if (message.fileDetails?.length) {
      obj.fileDetails = message.fileDetails.map((e) => FileIODetails.toJSON(e));
    }
    if (message.datastoreDetails?.length) {
      obj.datastoreDetails = message.datastoreDetails.map((e) => DatastoreIODetails.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<JobMetadata>, I>>(base?: I): JobMetadata {
    return JobMetadata.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<JobMetadata>, I>>(object: I): JobMetadata {
    const message = createBaseJobMetadata();
    message.sdkVersion = (object.sdkVersion !== undefined && object.sdkVersion !== null)
      ? SdkVersion.fromPartial(object.sdkVersion)
      : undefined;
    message.spannerDetails = object.spannerDetails?.map((e) => SpannerIODetails.fromPartial(e)) || [];
    message.bigqueryDetails = object.bigqueryDetails?.map((e) => BigQueryIODetails.fromPartial(e)) || [];
    message.bigTableDetails = object.bigTableDetails?.map((e) => BigTableIODetails.fromPartial(e)) || [];
    message.pubsubDetails = object.pubsubDetails?.map((e) => PubSubIODetails.fromPartial(e)) || [];
    message.fileDetails = object.fileDetails?.map((e) => FileIODetails.fromPartial(e)) || [];
    message.datastoreDetails = object.datastoreDetails?.map((e) => DatastoreIODetails.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExecutionStageState(): ExecutionStageState {
  return { executionStageName: "", executionStageState: 0, currentStateTime: undefined };
}

export const ExecutionStageState: MessageFns<ExecutionStageState> = {
  encode(message: ExecutionStageState, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.executionStageName !== "") {
      writer.uint32(10).string(message.executionStageName);
    }
    if (message.executionStageState !== 0) {
      writer.uint32(16).int32(message.executionStageState);
    }
    if (message.currentStateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.currentStateTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStageState {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStageState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.executionStageName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.executionStageState = reader.int32() as any;
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.currentStateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionStageState {
    return {
      executionStageName: isSet(object.executionStageName) ? globalThis.String(object.executionStageName) : "",
      executionStageState: isSet(object.executionStageState) ? jobStateFromJSON(object.executionStageState) : 0,
      currentStateTime: isSet(object.currentStateTime) ? fromJsonTimestamp(object.currentStateTime) : undefined,
    };
  },

  toJSON(message: ExecutionStageState): unknown {
    const obj: any = {};
    if (message.executionStageName !== "") {
      obj.executionStageName = message.executionStageName;
    }
    if (message.executionStageState !== 0) {
      obj.executionStageState = jobStateToJSON(message.executionStageState);
    }
    if (message.currentStateTime !== undefined) {
      obj.currentStateTime = message.currentStateTime.toISOString();
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ExecutionStageState>, I>>(base?: I): ExecutionStageState {
    return ExecutionStageState.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ExecutionStageState>, I>>(object: I): ExecutionStageState {
    const message = createBaseExecutionStageState();
    message.executionStageName = object.executionStageName ?? "";
    message.executionStageState = object.executionStageState ?? 0;
    message.currentStateTime = object.currentStateTime ?? undefined;
    return message;
  },
};

function createBaseJobExecutionInfo(): JobExecutionInfo {
  return { stages: {} };
}

export const JobExecutionInfo: MessageFns<JobExecutionInfo> = {
  encode(message: JobExecutionInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.stages).forEach(([key, value]) => {
      JobExecutionInfo_StagesEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          const entry1 = JobExecutionInfo_StagesEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.stages[entry1.key] = entry1.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionInfo {
    return {
      stages: isObject(object.stages)
        ? Object.entries(object.stages).reduce<{ [key: string]: JobExecutionStageInfo }>((acc, [key, value]) => {
          acc[key] = JobExecutionStageInfo.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: JobExecutionInfo): unknown {
    const obj: any = {};
    if (message.stages) {
      const entries = Object.entries(message.stages);
      if (entries.length > 0) {
        obj.stages = {};
        entries.forEach(([k, v]) => {
          obj.stages[k] = JobExecutionStageInfo.toJSON(v);
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<JobExecutionInfo>, I>>(base?: I): JobExecutionInfo {
    return JobExecutionInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<JobExecutionInfo>, I>>(object: I): JobExecutionInfo {
    const message = createBaseJobExecutionInfo();
    message.stages = Object.entries(object.stages ?? {}).reduce<{ [key: string]: JobExecutionStageInfo }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = JobExecutionStageInfo.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseJobExecutionInfo_StagesEntry(): JobExecutionInfo_StagesEntry {
  return { key: "", value: undefined };
}

export const JobExecutionInfo_StagesEntry: MessageFns<JobExecutionInfo_StagesEntry> = {
  encode(message: JobExecutionInfo_StagesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      JobExecutionStageInfo.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionInfo_StagesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionInfo_StagesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = JobExecutionStageInfo.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionInfo_StagesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? JobExecutionStageInfo.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: JobExecutionInfo_StagesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = JobExecutionStageInfo.toJSON(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<JobExecutionInfo_StagesEntry>, I>>(base?: I): JobExecutionInfo_StagesEntry {
    return JobExecutionInfo_StagesEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<JobExecutionInfo_StagesEntry>, I>>(object: I): JobExecutionInfo_StagesEntry {
    const message = createBaseJobExecutionInfo_StagesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? JobExecutionStageInfo.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseJobExecutionStageInfo(): JobExecutionStageInfo {
  return { stepName: [] };
}

export const JobExecutionStageInfo: MessageFns<JobExecutionStageInfo> = {
  encode(message: JobExecutionStageInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.stepName) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionStageInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionStageInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.stepName.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionStageInfo {
    return {
      stepName: globalThis.Array.isArray(object?.stepName) ? object.stepName.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: JobExecutionStageInfo): unknown {
    const obj: any = {};
    if (message.stepName?.length) {
      obj.stepName = message.stepName;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<JobExecutionStageInfo>, I>>(base?: I): JobExecutionStageInfo {
    return JobExecutionStageInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<JobExecutionStageInfo>, I>>(object: I): JobExecutionStageInfo {
    const message = createBaseJobExecutionStageInfo();
    message.stepName = object.stepName?.map((e) => e) || [];
    return message;
  },
};

function createBaseJobEventData(): JobEventData {
  return { payload: undefined };
}

export const JobEventData: MessageFns<JobEventData> = {
  encode(message: JobEventData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.payload !== undefined) {
      Job.encode(message.payload, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobEventData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobEventData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.payload = Job.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobEventData {
    return { payload: isSet(object.payload) ? Job.fromJSON(object.payload) : undefined };
  },

  toJSON(message: JobEventData): unknown {
    const obj: any = {};
    if (message.payload !== undefined) {
      obj.payload = Job.toJSON(message.payload);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<JobEventData>, I>>(base?: I): JobEventData {
    return JobEventData.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<JobEventData>, I>>(object: I): JobEventData {
    const message = createBaseJobEventData();
    message.payload = (object.payload !== undefined && object.payload !== null)
      ? Job.fromPartial(object.payload)
      : undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
